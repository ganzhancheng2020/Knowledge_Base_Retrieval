{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa9b845a-0a09-4cbd-9a5a-d3a737ccffb1",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.status.idle": "2024-11-15T12:54:47.414597Z",
     "shell.execute_reply": "2024-11-15T12:54:47.413920Z",
     "shell.execute_reply.started": "2024-11-15T12:54:36.301537Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.readers.file import PDFReader\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import Document\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.retrievers import AutoMergingRetriever\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.schema import TextNode, NodeWithScore\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "import json\n",
    "import Stemmer\n",
    "\n",
    "#模型下载\n",
    "from modelscope import snapshot_download\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pdfplumber\n",
    "\n",
    "import numpy as np\n",
    "from FlagEmbedding import FlagModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from zhipuLLM import GLM\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "from graph_rag import GraphRAGExtractor, GraphRAGStore\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbfc44ff-8f2e-48f0-a93d-85a162509a23",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-11-15T12:54:47.416691Z",
     "iopub.status.busy": "2024-11-15T12:54:47.415918Z",
     "iopub.status.idle": "2024-11-15T12:54:49.681932Z",
     "shell.execute_reply": "2024-11-15T12:54:49.681262Z",
     "shell.execute_reply.started": "2024-11-15T12:54:47.416665Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 20:54:47,419 - modelscope - WARNING - Authentication has expired, please re-login with modelscope login --token \"YOUR_SDK_TOKEN\" if you need to access private models or datasets.\n"
     ]
    }
   ],
   "source": [
    "model_dir = snapshot_download('AI-ModelScope/bge-large-zh-v1.5', cache_dir=\"./models\")\n",
    "rerank_dir = snapshot_download('AI-ModelScope/bge-reranker-v2-m3', cache_dir=\"./models\")\n",
    "# rerank_dir = snapshot_download('BAAI/bge-reranker-v2-minicpm-layerwise', cache_dir=\"./models\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(rerank_dir, use_fast=False, trust_remote_code=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(rerank_dir, trust_remote_code=True, torch_dtype=torch.bfloat16)\n",
    "#rerank_dir = \"./models/bge-reranker-v2-minicpm-layerwise\"\n",
    "reranker = SentenceTransformerRerank(model=rerank_dir, top_n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203a045e-dd16-4331-bf16-de6c401abcca",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "LLAMA_CLOUD_API_KEY=\"llx-0sSh6yWMfyrPI6sCnYbTPBqoWw63WidhqACRvlYF0VNIrJYh\"\n",
    "# bring in our LLAMA_CLOUD_API_KEY\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# bring in deps\n",
    "from llama_parse import LlamaParse\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# set up parser\n",
    "parser = LlamaParse(\n",
    "    result_type=\"markdown\",  # \"markdown\" and \"text\" are available\n",
    "    language=\"ch_sim\",\n",
    "    api_key=LLAMA_CLOUD_API_KEY\n",
    ")\n",
    "\n",
    "# use SimpleDirectoryReader to parse our file\n",
    "input_files=['/A_document/AF01.pdf']\n",
    "input_dir = \"./B_document\"\n",
    "file_extractor = {\".pdf\": parser}\n",
    "documents = SimpleDirectoryReader(input_dir=input_dir, file_extractor=file_extractor).load_data() #input_files=./A_documen/AF01.PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96f12ed4-cd13-4f79-9aba-0ab12dbfc79c",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-11-15T12:54:49.683394Z",
     "iopub.status.busy": "2024-11-15T12:54:49.682820Z",
     "iopub.status.idle": "2024-11-15T12:54:49.689691Z",
     "shell.execute_reply": "2024-11-15T12:54:49.689049Z",
     "shell.execute_reply.started": "2024-11-15T12:54:49.683369Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def doc_saver(documents, file_path):\n",
    "    \n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "    # 遍历字典列表中的每一个字典\n",
    "        for document in tqdm(documents):\n",
    "            # 将字典转换为JSON字符串，并写入文件\n",
    "            data_line = document.to_embedchain_format()\n",
    "            file.write(json.dumps(data_line, ensure_ascii=False) + '\\n')\n",
    "\n",
    "        print(f\"数据已成功写入 {file_path}\")\n",
    "        \n",
    "def doc_loader(file_path):\n",
    "    \n",
    "    # 创建一个空列表来存储解析后的字典\n",
    "    documents = []\n",
    "\n",
    "    # 打开文件，使用with语句确保文件正确关闭\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        # 逐行读取文件内容\n",
    "        for line in tqdm(file):\n",
    "            # 去除行尾的换行符，并将JSON字符串解析为字典\n",
    "            item = json.loads(line.strip())\n",
    "            # 将字典添加到列表中\n",
    "            doc = Document().from_embedchain_format(item)\n",
    "            documents.append(doc)\n",
    "            \n",
    "    return documents\n",
    "\n",
    "import glob\n",
    "def read_data(folder_path):\n",
    "    \n",
    "        # 初始化一个空列表来存储所有文件的内容\n",
    "    txt_files = glob.glob(os.path.join(folder_path, '*.txt'))\n",
    "    txt_content_list = []\n",
    "    \n",
    "    # 遍历每个 .txt 文件\n",
    "    for file_path in txt_files:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            # 读取文件内容并添加到列表中\n",
    "            content = file.read()\n",
    "            txt_content_list.append(content)\n",
    "    \n",
    "    return txt_content_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2142e625-202c-408e-aedb-1cc1617618be",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-11-15T12:54:50.603628Z",
     "iopub.status.busy": "2024-11-15T12:54:50.603222Z",
     "iopub.status.idle": "2024-11-15T12:54:50.609310Z",
     "shell.execute_reply": "2024-11-15T12:54:50.608683Z",
     "shell.execute_reply.started": "2024-11-15T12:54:50.603597Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def filter_text(text):\n",
    "    # '\\n', ' ' 删除\n",
    "    text = text.replace(' ','')\n",
    "    # 删除页码\n",
    "    # 删除本文档为2024CCFBDC***\n",
    "    head_pattern = '本文档为2024CCFBDCI比赛用语料的一部分。[\\s\\S]+仅允许在本次比赛中使用。'\n",
    "    # news_pattern\n",
    "    pattern1 = r\"发布时间：[\\s\\S]+发布人：新闻宣传中心\"\n",
    "    pattern2 = r\"发布时间：[\\s\\S]+发布人：新闻发布人\"\n",
    "    pattern3 =  r'发布时间：\\d{4}年\\d{1,2}月\\d{1,2}日'\n",
    "    news_pattern = head_pattern+'|'+pattern1+'|'+pattern2+'|'+pattern3\n",
    "    text = re.sub(news_pattern,'',text)\n",
    "    # report_pattern\n",
    "    report_pattern1 = '第一节重要提示[\\s\\S]+本次利润分配方案尚需提交本公司股东大会审议。'\n",
    "    report_pattern12 = '一重要提示[\\s\\S]+股东大会审议。'\n",
    "    report_pattern13 = '一、重要提示[\\s\\S]+季度报告未经审计。'\n",
    "    report_pattern2 = '本公司董事会及全体董事保证本公告内容不存在任何虚假记载、[\\s\\S]+季度财务报表是否经审计[\\s\\S]+□是√否'\n",
    "    report_pattern3 = '中国联合网络通信股份有限公司（简称“公司”）董事会审计委员会根据相关法律法规、[\\s\\S]+汇报如下：'\n",
    "    report_pattern = report_pattern1+'|'+report_pattern12+'|'+report_pattern13+'|'+report_pattern2+'|'+report_pattern3\n",
    "    text = re.sub( report_pattern,'',text)\n",
    "    \n",
    "#     text = text.replace('(','（').replace(')','）')\n",
    "#     first_line = text.split('\\n')[0].strip()\n",
    "#     if first_line.startswith('#No'):\n",
    "#         process_text = text.replace(first_line ,'')\n",
    "#         text = re.sub(r'^#(.*?)\\n\\n#(.*?)\\n', r'#\\1\\2\\n', process_text, flags=re.MULTILINE | re.DOTALL).strip()\n",
    "#     else:\n",
    "#         text = re.sub(r'^#(.*?)\\n\\n#(.*?)\\n', r'#\\1\\2\\n', text, flags=re.MULTILINE | re.DOTALL).strip()\n",
    "#         # 使用正则表达式获取第一行内容\n",
    "#     first_line = text.split('\\n')[0].strip()\n",
    "#     final_text = first_line + text.replace(first_line, '')\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89990ab-27a2-4dd7-a348-eeb668c2c8bd",
   "metadata": {},
   "source": [
    "### load file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f798acf-d956-4f11-8778-1f273b08bddc",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-11-15T12:54:52.979465Z",
     "iopub.status.busy": "2024-11-15T12:54:52.979076Z",
     "iopub.status.idle": "2024-11-15T12:54:53.061922Z",
     "shell.execute_reply": "2024-11-15T12:54:53.061334Z",
     "shell.execute_reply.started": "2024-11-15T12:54:52.979440Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "718it [00:00, 23595.26it/s]\n",
      "1062it [00:00, 23941.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# c_texts = read_data(\"./datas/data2/output_txt\")\n",
    "# documents = [Document(text=filter_text(text)) for text in c_texts]\n",
    "file_path_a = \"doc.jsonl\"\n",
    "file_path_b = \"doc_b.jsonl\"\n",
    "#doc_saver(documents, file_path)\n",
    "documents = doc_loader(file_path_b) + doc_loader(file_path_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7e13aa0-d66c-48cc-b851-bb1224b42b4c",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-11-15T12:54:57.423321Z",
     "iopub.status.busy": "2024-11-15T12:54:57.422937Z",
     "iopub.status.idle": "2024-11-15T12:54:57.478086Z",
     "shell.execute_reply": "2024-11-15T12:54:57.477421Z",
     "shell.execute_reply.started": "2024-11-15T12:54:57.423285Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# 使用 defaultdict 来收集具有相同 file_name 的文档\n",
    "document_dict = defaultdict(list)\n",
    "\n",
    "for doc in documents:\n",
    "    document_dict[doc.metadata['file_name']].append(doc)\n",
    "\n",
    "# 合并具有相同 file_name 的文档\n",
    "merged_documents = []\n",
    "for file_name, docs in document_dict.items():\n",
    "    merged_text = docs[0].text.strip()\n",
    "    for i in range(1, len(docs)):\n",
    "        if merged_text.endswith('。') or docs[i].text.startswith('#'):\n",
    "        # 如果前一个字符串以句号结尾，或当前字符串以#开头，用两个换行符拼接\n",
    "            merged_text += '\\n\\n' + docs[i].text.strip()\n",
    "        else:\n",
    "        # 否则直接拼接\n",
    "             merged_text += docs[i].text.strip()\n",
    "        # 合并文本内容\n",
    "        \n",
    "    publish_time_match = re.search(r'发布时间：(\\d{4}-\\d{2}-\\d{2})', merged_text)\n",
    "    publisher_match = re.search(r'发布人：(\\S+)', merged_text)\n",
    "    merged_text = filter_text(merged_text)\n",
    "        \n",
    "    # 使用第一个文档的元数据作为合并后的文档元数据\n",
    "    merged_metadata = docs[0].metadata.copy()\n",
    "    # 提取元数据key\n",
    "    ori_metadata = [key for key in docs[0].metadata.keys()]\n",
    "    # 提取发布时间和发布人\n",
    "    #title_match = re.search(r'#\\s*(.+?)\\n', merged_text)\n",
    "        \n",
    "    merged_metadata['发布时间'] = publish_time_match.group(1) if publish_time_match else ''\n",
    "    merged_metadata['发布人'] = publisher_match.group(1) if publisher_match else ''\n",
    "        #merged_metadata['title'] = title_match.group(1) if title_match else ''\n",
    "    \n",
    "    merged_doc = Document(\n",
    "        id_=docs[0].id_,\n",
    "        embedding=None,\n",
    "        metadata=merged_metadata,\n",
    "        excluded_embed_metadata_keys=ori_metadata,\n",
    "        excluded_llm_metadata_keys=ori_metadata,\n",
    "        relationships={},\n",
    "        text=merged_text,\n",
    "        mimetype='text/plain',\n",
    "        start_char_idx=None,\n",
    "        end_char_idx=None,\n",
    "        text_template='{metadata_str}\\n\\n{content}',\n",
    "        metadata_template='{key}: {value}',\n",
    "        metadata_seperator='\\n'\n",
    "    )\n",
    "    merged_documents.append(merged_doc)\n",
    "documents = merged_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b141da7d-2efd-4462-baee-872948639aaa",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-11-15T12:55:01.850902Z",
     "iopub.status.busy": "2024-11-15T12:55:01.850528Z",
     "iopub.status.idle": "2024-11-15T12:55:01.966322Z",
     "shell.execute_reply": "2024-11-15T12:55:01.965626Z",
     "shell.execute_reply.started": "2024-11-15T12:55:01.850878Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "def chinese_tokenizer(text: str) -> list[str]:\n",
    "    try:\n",
    "        # 加载停用词\n",
    "        stopwords = set()\n",
    "        if os.path.exists('stopwords.txt'):\n",
    "            with open('stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "                stopwords = set(line.strip() for line in f)\n",
    "\n",
    "        # 分词并过滤停用词\n",
    "        tokens = jieba.lcut(text)\n",
    "        return [token for token in tokens if token not in stopwords]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in tokenization: {e}\")\n",
    "        return jieba.lcut(text)  # 降级处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74b0dd13-0356-42f7-8558-8d548b5ed72e",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-11-15T12:55:17.133692Z",
     "iopub.status.busy": "2024-11-15T12:55:17.133284Z",
     "iopub.status.idle": "2024-11-15T12:55:18.933397Z",
     "shell.execute_reply": "2024-11-15T12:55:18.932655Z",
     "shell.execute_reply.started": "2024-11-15T12:55:17.133667Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in EntityExtractor has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.extractors import (\n",
    "    SummaryExtractor,\n",
    "    QuestionsAnsweredExtractor,\n",
    "    TitleExtractor,\n",
    "    KeywordExtractor,\n",
    "    BaseExtractor,\n",
    ")\n",
    "from llama_index.extractors.entity import EntityExtractor\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.openai_like import OpenAILike as OpenAI\n",
    "#from llama_index.llms.openllm import OpenLLM as OpenAI\n",
    "#from llama_index.legacy.llms import OpenAILike as OpenAI\n",
    "from llama_index.legacy.callbacks import CallbackManager\n",
    "from llama_index.core import Settings\n",
    "from dotenv import dotenv_values\n",
    "import os\n",
    "import nltk\n",
    "\n",
    "config = dotenv_values(\".env\")\n",
    "glm_api_key = config[\"glm_api_key\"]\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "\n",
    "callback_manager = CallbackManager()\n",
    "\n",
    "text_splitter = SentenceSplitter(chunk_size=516, chunk_overlap=128, tokenizer=chinese_tokenizer)\n",
    "\n",
    "llm = OpenAI(\n",
    "        model=\"glm-4-plus\",\n",
    "        api_base=\"https://open.bigmodel.cn/api/paas/v4\",\n",
    "        api_key=glm_api_key,\n",
    "        callback_manager=callback_manager,\n",
    "        is_chat_model=True\n",
    "    )\n",
    "Settings.llm = llm\n",
    "\n",
    "QA_template = \"\"\"\\\n",
    "以下是上下文信息：\n",
    "{context_str}\n",
    "\n",
    "请根据上述上下文信息，生成 {num_questions} 个问题。这些问题的具体答案应主要来源于上下文，而不是其他地方。\n",
    "利用这些上下文信息，生成有深度和相关性的问题，这些问题能够由提供的上下文信息准确回答。\n",
    "注意，请直接生成问题即可，无需额外解释。\n",
    "\"\"\"\n",
    "\n",
    "TITLE_NODE_TEMPLATE = \"\"\"\\\n",
    "以下是上下文信息：\n",
    "{context_str}\n",
    "\n",
    "给出一个标题，该标题总结上下文中所有独特的实体、标题或主题。\n",
    "注意，直接生成标题即可，无需任何额外解释。\n",
    "标题:\n",
    "\"\"\"\n",
    "\n",
    "TITLE_COMBINE_TEMPLATE = \"\"\"\\\n",
    "{context_str}\n",
    "\n",
    "基于上述候选标题和内容，输出这份文档的综合标题。\n",
    "注意，直接生成标题即可，无需任何额外解释。\n",
    "标题:\n",
    "\"\"\"\n",
    "\n",
    "KEYWORD_EXTRACT_TEMPLATE = \"\"\"\n",
    "{context_str}\n",
    "为这份文档提供{keywords}个独特的关键词,并以逗号(,)分隔。\n",
    "\n",
    "示例:\n",
    "关键词, 关键词, 关键词...\n",
    "\"\"\"\n",
    "\n",
    "extractors = [\n",
    "    TitleExtractor(nodes=1, node_template=TITLE_NODE_TEMPLATE, combine_template=TITLE_COMBINE_TEMPLATE, llm=GLM()),\n",
    "    QuestionsAnsweredExtractor(questions=3, prompt_template=QA_template,llm=GLM()),\n",
    "    #EntityExtractor(model_name=\"span-marker\", prediction_threshold=0.5, label_entities=True, device=\"cuda\"),\n",
    "    # SummaryExtractor(summaries=[\"prev\", \"self\"], llm=llm),\n",
    "    KeywordExtractor(keywords=10, prompt_template=KEYWORD_EXTRACT_TEMPLATE, llm=GLM()),\n",
    "    # CustomExtractor()\n",
    "]\n",
    "\n",
    "transformations = [text_splitter] + extractors\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "pipeline = IngestionPipeline(transformations=transformations)\n",
    "\n",
    "# uber_nodes = pipeline.run(documents=documents)\n",
    "# base_index = VectorStoreIndex(uber_nodes)\n",
    "# base_index.storage_context.persist(persist_dir=\"./storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aed28544-49c1-474f-be4f-a4b4c661ca7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T12:55:23.409236Z",
     "iopub.status.busy": "2024-11-15T12:55:23.408855Z",
     "iopub.status.idle": "2024-11-15T12:55:26.185742Z",
     "shell.execute_reply": "2024-11-15T12:55:26.185007Z",
     "shell.execute_reply.started": "2024-11-15T12:55:23.409212Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 使用本地模型路径创建 HuggingFaceEmbedding 实例\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=model_dir,  # 使用本地模型路径\n",
    "    cache_folder=model_dir,  # 指定缓存目录\n",
    "    embed_batch_size=512\n",
    ")\n",
    "\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10e5baa-54f6-4efe-866d-4b7839b0fa04",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# 获取当前时间并格式化\n",
    "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "persist_dir= './storage/20241110_192022' #f\"./storage/{current_time}\"\n",
    "\n",
    "for docs in tqdm(documents):\n",
    "    nodes = pipeline.run(documents=[docs])\n",
    "    if os.path.exists(persist_dir):\n",
    "        base_index = load_index_from_storage(\n",
    "            StorageContext.from_defaults(persist_dir=persist_dir)\n",
    "        )\n",
    "        base_index.insert_nodes(nodes)\n",
    "    else:\n",
    "        base_index = VectorStoreIndex(nodes)\n",
    "    base_index.storage_context.persist(persist_dir=persist_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dced0f8a-5b08-4692-b0aa-1edfd9bc470a",
   "metadata": {},
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00602702-3b0d-4d97-ad56-e0e376c35cf1",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-11-02T09:11:50.375989Z",
     "iopub.status.busy": "2024-11-02T09:11:50.375633Z",
     "iopub.status.idle": "2024-11-02T09:11:50.379590Z",
     "shell.execute_reply": "2024-11-02T09:11:50.378996Z",
     "shell.execute_reply.started": "2024-11-02T09:11:50.375939Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 指定文件夹路径\n",
    "# input_dir = \"./A_document\"\n",
    "\n",
    "# # 初始化一个字典来存储每个 PDF 文件的文本\n",
    "# pdf_texts = {}\n",
    "\n",
    "# # 遍历文件夹中的所有文件\n",
    "# for filename in tqdm(os.listdir(input_dir)):\n",
    "#     if filename.endswith(\".pdf\"):\n",
    "#         file_path = os.path.join(input_dir, filename)\n",
    "        \n",
    "#         # 打开 PDF 文件\n",
    "#         with pdfplumber.open(file_path) as pdf:\n",
    "#             whole_text = ''\n",
    "#             # 遍历每一页\n",
    "#             for page in pdf.pages:\n",
    "#                 # 提取文本\n",
    "#                 text = page.extract_text()\n",
    "#                 if text:\n",
    "#                     whole_text += text + '\\n'\n",
    "        \n",
    "#             # 去掉第一句话，并将第二句话作为文章标题\n",
    "#             sentences = whole_text.split('。')\n",
    "#             if len(sentences) > 1:\n",
    "#                 content = '。'.join(sentences[1:])  # 从第三句话开始拼接内容\n",
    "#                 content = whole_text\n",
    "#         # 存储文本\n",
    "#         pdf_texts[filename] = whole_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767ff204-de66-4995-bc56-c15a4d8b72d6",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376b0afb-073c-434f-bbf8-2c58ce1c8c19",
   "metadata": {},
   "source": [
    "### Set embed_model and LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd8ea83d-8b5e-4077-a9e8-6f6f4b9bf2cb",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-11-13T13:29:33.677324Z",
     "iopub.status.busy": "2024-11-13T13:29:33.676882Z",
     "iopub.status.idle": "2024-11-13T13:29:34.210682Z",
     "shell.execute_reply": "2024-11-13T13:29:34.209228Z",
     "shell.execute_reply.started": "2024-11-13T13:29:33.677296Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 使用本地模型路径创建 HuggingFaceEmbedding 实例\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=model_dir,  # 使用本地模型路径\n",
    "    cache_folder=model_dir,  # 指定缓存目录\n",
    "    embed_batch_size=512\n",
    ")\n",
    "\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a14911-2133-4f85-8bbb-d403ae48314c",
   "metadata": {},
   "source": [
    "### Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5d387eb-a2fe-43c7-884e-63e04fe578cc",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-11-15T12:55:36.348075Z",
     "iopub.status.busy": "2024-11-15T12:55:36.347688Z",
     "iopub.status.idle": "2024-11-15T12:55:36.354184Z",
     "shell.execute_reply": "2024-11-15T12:55:36.353494Z",
     "shell.execute_reply.started": "2024-11-15T12:55:36.348052Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(vec_a, vec_b):\n",
    "    dot_product = np.dot(vec_a, vec_b)\n",
    "    norm_a = np.linalg.norm(vec_a)\n",
    "    norm_b = np.linalg.norm(vec_b)\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "\n",
    "def get_similarity(sentences_1, sentences_2):\n",
    "    sentences_1 = sentences_1\n",
    "    sentences_2 = sentences_2\n",
    "    model = SentenceTransformer(model_dir)\n",
    "    embeddings_1 = model.encode(sentences_1, normalize_embeddings=True)\n",
    "    embeddings_2 = model.encode(sentences_2, normalize_embeddings=True)\n",
    "    similarity = embeddings_1 @ embeddings_2.T\n",
    "    return similarity\n",
    "\n",
    "\n",
    "def get_scores(query, prompt):\n",
    "    queries = [query]\n",
    "    passages = prompt\n",
    "    instruction = \"为这个句子生成表示以用于检索相关文章：\"\n",
    "\n",
    "    model = SentenceTransformer(model_dir)\n",
    "    q_embeddings = model.encode([instruction+q for q in queries], normalize_embeddings=True)\n",
    "    p_embeddings = model.encode(passages, normalize_embeddings=True)\n",
    "    scores = q_embeddings @ p_embeddings.T\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846235a5-16d8-44a3-b1ba-650a70475acd",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-10-09T15:55:59.719706Z",
     "iopub.status.busy": "2024-10-09T15:55:59.719379Z",
     "iopub.status.idle": "2024-10-09T15:55:59.741998Z",
     "shell.execute_reply": "2024-10-09T15:55:59.741428Z",
     "shell.execute_reply.started": "2024-10-09T15:55:59.719687Z"
    },
    "tags": []
   },
   "source": [
    "### Base Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51420366-cf88-41d4-b0ee-3bee3278fd89",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-11-15T12:55:38.799567Z",
     "iopub.status.busy": "2024-11-15T12:55:38.799138Z",
     "iopub.status.idle": "2024-11-15T12:56:51.884379Z",
     "shell.execute_reply": "2024-11-15T12:56:51.883668Z",
     "shell.execute_reply.started": "2024-11-15T12:55:38.799541Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.785 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "## base_index\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "\n",
    "# text_splitter = TokenTextSplitter(\n",
    "#     separator=\"。\", chunk_size=1024, chunk_overlap=128\n",
    "# )\n",
    "\n",
    "text_splitter = SentenceSplitter(chunk_size=516, chunk_overlap=128, tokenizer=chinese_tokenizer)\n",
    "base_nodes = text_splitter.get_nodes_from_documents(documents)\n",
    "base_index = VectorStoreIndex(base_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33eed08-0e59-4cb0-a036-07e86b208d41",
   "metadata": {},
   "source": [
    "### Auto_retriever_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26ea703f-c9a3-4e45-940b-aa648894492e",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-11-15T12:56:51.885929Z",
     "iopub.status.busy": "2024-11-15T12:56:51.885631Z",
     "iopub.status.idle": "2024-11-15T12:59:15.604962Z",
     "shell.execute_reply": "2024-11-15T12:59:15.604330Z",
     "shell.execute_reply.started": "2024-11-15T12:56:51.885906Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Load index into vector index\n",
    "torch.cuda.empty_cache()\n",
    "from llama_index.core.node_parser import (\n",
    "    HierarchicalNodeParser,\n",
    "    SentenceSplitter,\n",
    ")\n",
    "from llama_index.core.node_parser import get_leaf_nodes, get_root_nodes\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "\n",
    "node_parser = HierarchicalNodeParser.from_defaults(chunk_sizes = [2048, 1024, 256])\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "leaf_nodes = get_leaf_nodes(nodes)\n",
    "root_nodes = get_root_nodes(nodes)\n",
    "\n",
    "auto_retriever_index = VectorStoreIndex(leaf_nodes)\n",
    "\n",
    "docstore = SimpleDocumentStore()\n",
    "# insert nodes into docstore\n",
    "docstore.add_documents(nodes)\n",
    "# define storage context (will include vector store by default too)\n",
    "storage_context = StorageContext.from_defaults(docstore=docstore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c71113-7932-4939-8e03-32d0479b1b27",
   "metadata": {},
   "source": [
    "### SentenceWindowIndexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7859f77f-b9ce-4594-a4be-730f46ff548d",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-11-15T12:59:15.606179Z",
     "iopub.status.busy": "2024-11-15T12:59:15.605948Z",
     "iopub.status.idle": "2024-11-15T13:06:50.020854Z",
     "shell.execute_reply": "2024-11-15T13:06:50.020169Z",
     "shell.execute_reply.started": "2024-11-15T12:59:15.606157Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "for d in documents:\n",
    "    d.text = d.text.replace('。','. ')\n",
    "    d.text = d.text.replace('！','! ')\n",
    "    d.text = d.text.replace('？','? ')\n",
    "    d.text = d.text.replace('，',', ')\n",
    "    \n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    " \n",
    "#定义句子解析器\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=3,\n",
    "    window_metadata_key=\"window\",\n",
    "    original_text_metadata_key=\"original_text\",\n",
    ")\n",
    "\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "sentence_index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf64bc60-817f-4d6c-b18a-8214024781ba",
   "metadata": {},
   "source": [
    "### PropertyGraphIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8e0c39-a44c-4f1e-994c-a527fb75ca85",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import PropertyGraphIndex\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.indices.property_graph import (\n",
    "    ImplicitPathExtractor,\n",
    "    SimpleLLMPathExtractor,\n",
    ")\n",
    "#from llama_index.llms.dashscope import DashScope\n",
    "\n",
    "import nest_asyncio\n",
    "import os\n",
    "nest_asyncio.apply()\n",
    "\n",
    "#os.environ[\"DASHSCOPE_API_KEY\"] = \"c173736660e1fc3a20d7f2ad1f368064.ywcM3QDFttUwKpul\"\n",
    "# graph_index = PropertyGraphIndex.from_documents(\n",
    "#     documents=documents,\n",
    "#     llm=DashScope(\n",
    "#         model = \"qwen-plus-latest\",\n",
    "#         base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "#         api_key=\"sk-9fe47a41bf8742dc908ce651fd8261a2\",\n",
    "#         is_chat_model=True,\n",
    "#     ),\n",
    "#     embed_model=embed_model,\n",
    "#     show_progress=True\n",
    "# )\n",
    "text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=128, tokenizer=chinese_tokenizer)\n",
    "extractors = [\n",
    "    TitleExtractor(nodes=5, llm=llm),\n",
    "    QuestionsAnsweredExtractor(questions=3, prompt_template=QA_template,llm=llm),\n",
    "    KeywordExtractor(keywords=10, llm=llm),\n",
    "]\n",
    "\n",
    "transformations = [text_splitter] + extractors\n",
    "\n",
    "graph_index = PropertyGraphIndex.from_documents(\n",
    "    documents=documents,\n",
    "    transformations=transformations,\n",
    "    kg_extractors=[\n",
    "        SimpleLLMPathExtractor(\n",
    "            llm=llm,\n",
    "            num_workers=5,\n",
    "            max_paths_per_chunk=20,\n",
    "        ),\n",
    "    ],\n",
    "    llm=llm,\n",
    "    embed_model=embed_model,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "# graph_index.storage_context.persist(persist_dir=\"./storage\")\n",
    "# graph_index.property_graph_store.save_networkx_graph(name=\"./中国联通.html\")\n",
    "# base_url = \"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "# api_key = \"sk-9fe47a41bf8742dc908ce651fd8261a2\", # 随便填写，只是为了通过接口参数校验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b715ca1c-a186-4477-ba8d-48cddd74a123",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T13:06:50.022862Z",
     "iopub.status.busy": "2024-11-15T13:06:50.022502Z",
     "iopub.status.idle": "2024-11-15T13:07:03.384127Z",
     "shell.execute_reply": "2024-11-15T13:07:03.383474Z",
     "shell.execute_reply.started": "2024-11-15T13:06:50.022837Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "persist_dir = './storage/20241110_192022'\n",
    "graph_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=persist_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3290a6ec-36d2-486a-9585-e226cfe14e18",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-11-13T13:36:45.022080Z",
     "iopub.status.busy": "2024-11-13T13:36:45.021639Z",
     "iopub.status.idle": "2024-11-13T13:36:45.085405Z",
     "shell.execute_reply": "2024-11-13T13:36:45.084524Z",
     "shell.execute_reply.started": "2024-11-13T13:36:45.022055Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'uber_nodes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 21\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m entities, relationships\n\u001b[1;32m     12\u001b[0m kg_extractor \u001b[38;5;241m=\u001b[39m GraphRAGExtractor(\n\u001b[1;32m     13\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m     14\u001b[0m     extract_prompt\u001b[38;5;241m=\u001b[39mKG_TRIPLET_EXTRACT_TMPL,\n\u001b[1;32m     15\u001b[0m     max_paths_per_chunk\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     16\u001b[0m     parse_fn\u001b[38;5;241m=\u001b[39mparse_fn,\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     20\u001b[0m graph_index \u001b[38;5;241m=\u001b[39m PropertyGraphIndex(\n\u001b[0;32m---> 21\u001b[0m     nodes\u001b[38;5;241m=\u001b[39m\u001b[43muber_nodes\u001b[49m,\n\u001b[1;32m     22\u001b[0m     property_graph_store\u001b[38;5;241m=\u001b[39mGraphRAGStore(),\n\u001b[1;32m     23\u001b[0m     kg_extractors\u001b[38;5;241m=\u001b[39m[kg_extractor],\n\u001b[1;32m     24\u001b[0m     show_progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     25\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'uber_nodes' is not defined"
     ]
    }
   ],
   "source": [
    "from graph_rag import KG_TRIPLET_EXTRACT_TMPL\n",
    "from llama_index.core import PropertyGraphIndex\n",
    "\n",
    "def parse_fn(response_str: str) -> any:\n",
    "    entity_pattern = r'\\(\"entity\"\\$\\$\\$\\$\"(.+?)\"\\$\\$\\$\\$\"(.+?)\"\\$\\$\\$\\$\"(.+?)\"\\)'\n",
    "    relationship_pattern = r'\\(\"relationship\"\\$\\$\\$\\$\"(.+?)\"\\$\\$\\$\\$\"(.+?)\"\\$\\$\\$\\$\"(.+?)\"\\$\\$\\$\\$\"(.+?)\"\\)'\n",
    "    entities = re.findall(entity_pattern, response_str)\n",
    "    relationships = re.findall(relationship_pattern, response_str)\n",
    "    return entities, relationships\n",
    "\n",
    "\n",
    "kg_extractor = GraphRAGExtractor(\n",
    "    llm=llm,\n",
    "    extract_prompt=KG_TRIPLET_EXTRACT_TMPL,\n",
    "    max_paths_per_chunk=2,\n",
    "    parse_fn=parse_fn,\n",
    ")\n",
    "\n",
    "\n",
    "graph_index = PropertyGraphIndex(\n",
    "    nodes=uber_nodes,\n",
    "    property_graph_store=GraphRAGStore(),\n",
    "    kg_extractors=[kg_extractor],\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428a64bb-c382-4e67-9dc7-cd6416da6d82",
   "metadata": {},
   "source": [
    "### Load graph_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877755ce-7c2f-4775-9ef8-29b77c9add00",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.status.busy": "2024-11-12T13:51:41.452323Z",
     "iopub.status.idle": "2024-11-12T13:51:41.452525Z",
     "shell.execute_reply": "2024-11-12T13:51:41.452435Z",
     "shell.execute_reply.started": "2024-11-12T13:51:41.452426Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "graph_index.storage_context.persist(persist_dir=\"./storage\")\n",
    "graph_index.property_graph_store.save_networkx_graph(name=\"./中国联通.html\")\n",
    "graph_index = load_index_from_storage(\n",
    "    StorageContext.from_defaults(persist_dir=\"./storage\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77074d1a-43e8-4909-906f-418eb6784609",
   "metadata": {},
   "source": [
    "### Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9cdcc7e0-67ee-4cad-a61d-f344faac33fb",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-11-15T15:27:42.942442Z",
     "iopub.status.busy": "2024-11-15T15:27:42.942047Z",
     "iopub.status.idle": "2024-11-15T15:27:42.949947Z",
     "shell.execute_reply": "2024-11-15T15:27:42.949339Z",
     "shell.execute_reply.started": "2024-11-15T15:27:42.942418Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def window_retriever(query, topk):\n",
    "    sentence_window_retriever = sentence_index.as_retriever(similarity_top_k=topk)\n",
    "    nodes = sentence_window_retriever.retrieve(query)\n",
    "    #answer = [node.metadata['window'] for node in nodes]\n",
    "\n",
    "    return nodes #answer\n",
    "\n",
    "def base_vector_retriever(query, topk):\n",
    "    base_retriever = base_index.as_retriever(similarity_top_k=topk)\n",
    "    nodes = base_retriever.retrieve(query)\n",
    "    #answer = [node.text for node in nodes]\n",
    "\n",
    "    return nodes #answer\n",
    "\n",
    "def get_bm25_retriever(query, topk):\n",
    "# We can pass in the index, docstore, or list of nodes to create the retriever\n",
    "    bm25_retriever = BM25Retriever.from_defaults(\n",
    "        nodes=base_nodes,\n",
    "        similarity_top_k=topk,\n",
    "        # Optional: We can pass in the stemmer and set the language for stopwords\n",
    "        # This is important for removing stopwords and stemming the query + text\n",
    "        # The default is english for both\n",
    "        tokenizer=chinese_tokenizer\n",
    "    )\n",
    "\n",
    "    nodes = bm25_retriever.retrieve(query)\n",
    "    #answer = [node.text for node in nodes]\n",
    "    return nodes #answer\n",
    "\n",
    "def auto_retriever(query, topk):\n",
    "    base_retriever = auto_retriever_index.as_retriever(similarity_top_k=topk)\n",
    "    auto_retriever = AutoMergingRetriever(base_retriever, storage_context, verbose=False)\n",
    "    \n",
    "    nodes = auto_retriever.retrieve(query)\n",
    "    #answer = [node.text for node in nodes]\n",
    "    return nodes#answer\n",
    "\n",
    "def graph_retriever(query, topk):\n",
    "    \n",
    "    retriever = graph_index.as_retriever(similarity_top_k=topk)\n",
    "    nodes = retriever.retrieve(query)\n",
    "    #answer = [node.text for node in nodes]\n",
    "\n",
    "    return nodes #answer\n",
    "\n",
    "def Hybrid_retriever(query, topk=25):\n",
    "    retrievers = [graph_retriever, auto_retriever, get_bm25_retriever, window_retriever]# retriever1, auto_retriever, get_bm25_retriever, retriever2, auto_retriever\n",
    "    \n",
    "    nodes = []\n",
    "    for retriever in retrievers:\n",
    "        nodes.extend(retriever(query, topk))\n",
    "    \n",
    "    # combine the two lists of nodes\n",
    "    all_nodes = []\n",
    "    node_ids = set()\n",
    "    for n in nodes:\n",
    "        if n.node.node_id not in node_ids:\n",
    "            all_nodes.append(n)\n",
    "            node_ids.add(n.node.node_id)\n",
    "    \n",
    "    return all_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b2b38e-5b4d-4b87-a92c-45c5128437db",
   "metadata": {},
   "source": [
    "## Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "85978bc4-44f4-4059-87d8-0bfe9e34a9d4",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-11-15T15:34:09.611074Z",
     "iopub.status.busy": "2024-11-15T15:34:09.610671Z",
     "iopub.status.idle": "2024-11-15T15:34:09.620111Z",
     "shell.execute_reply": "2024-11-15T15:34:09.619525Z",
     "shell.execute_reply.started": "2024-11-15T15:34:09.611048Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def get_rerank(answers, threshold=0.2):\n",
    "    \n",
    "    # 调用 get_similarity 函数计算相似度矩阵\n",
    "    similarity_matrix = get_similarity(answers, answers)\n",
    "\n",
    "    # 将每个句子的相似度之和保存到列表中，排除自相似（对角线）\n",
    "    similarity_sums = np.sum(similarity_matrix, axis=1) - np.diag(similarity_matrix)\n",
    "    #print(similarity_matrix)\n",
    "\n",
    "     # 找到最大相似度之和\n",
    "    max_similarity_sum = np.max(similarity_sums)\n",
    "\n",
    "    # 计算每个句子与最大值的差值，并筛选出差值在 threshold 范围内的句子\n",
    "    reranked_answer = [(answers[i], similarity_sums[i]) for i in range(len(answers))\n",
    "                    if abs(similarity_sums[i] - max_similarity_sum) <= threshold]\n",
    "\n",
    "    # 按 similarity_sums 从高到低排序，并提取句子\n",
    "    result = [sentence for sentence, _ in sorted(reranked_answer, key=lambda x: x[1], reverse=True)]\n",
    "    print(f\"num of chunks: {len(result)}\")\n",
    "    return result\n",
    "\n",
    "def get_rerank2(query, answers):\n",
    "    \n",
    "    # max_score = 0\n",
    "    # final_answers = answers[0]\n",
    "    # for answer in answers:\n",
    "    #     score = get_scores(query, answer)\n",
    "    #     if max_score > score:\n",
    "    #         final_answers = answer\n",
    "    #         max_score = score\n",
    "    scores = get_scores(query, answers)\n",
    "    idx = np.argmax(scores[0])\n",
    "    print(scores[0][idx])\n",
    "    final_answers = answers[idx]\n",
    "            \n",
    "    return [final_answers] \n",
    "\n",
    "def get_rerank3(query, answers):\n",
    "    q = QueryBundle(query)\n",
    "    nodes = answers\n",
    "    # nodes = []\n",
    "    # for i in answers:\n",
    "    #     n = NodeWithScore(node=TextNode(text=i))\n",
    "    #     nodes.append(n)\n",
    "    nodes = [node for node in nodes if (not node.text.startswith('#')) and len(node.text)>40]\n",
    "    reranked_nodes = reranker.postprocess_nodes(nodes, q)\n",
    "    max_score = reranked_nodes[0].score\n",
    "    # result = [reranked_nodes[0].text]\n",
    "    \n",
    "    result = []\n",
    "    rerank_node=[]\n",
    "    for node in reranked_nodes:\n",
    "        text = node.text\n",
    "        text = text.replace('. ','。').replace('! ','！').replace('? ','？').replace(', ','，').replace(' ','').strip()\n",
    "        text = re.sub(r'(?<=[\\u4e00-\\u9fff])\\n(?=[\\u4e00-\\u9fff])', '', text)\n",
    "        if (node.score > max_score - 0.05 and text not in result):\n",
    "            result.append(text)\n",
    "            rerank_node.append(node)\n",
    "            #print(node.score, text)\n",
    "    \n",
    "    return rerank_node[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b57f3a-f613-4da8-a2dc-594d14a4feca",
   "metadata": {},
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ea5d0554-1e1f-4850-b52c-e45b2b813949",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-11-15T15:34:13.077773Z",
     "iopub.status.busy": "2024-11-15T15:34:13.077384Z",
     "iopub.status.idle": "2024-11-15T15:34:13.084610Z",
     "shell.execute_reply": "2024-11-15T15:34:13.083977Z",
     "shell.execute_reply.started": "2024-11-15T15:34:13.077749Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_merge(text_list):\n",
    "    \"\"\"\n",
    "    合并一个文本列表中的所有文本块，其中每个文本块的开头部分可能与前一个文本块的结尾部分重叠，\n",
    "    或者每个文本块的结尾部分可能与下一个文本块的开头部分重叠。\n",
    "\n",
    "    :param text_list: 文本列表\n",
    "    :return: 合并后的文本\n",
    "    \"\"\"\n",
    "    if not text_list:\n",
    "        return \"\"\n",
    "\n",
    "    # 初始化合并后的文本为第一个文本块\n",
    "    merged_text = text_list[0]\n",
    "\n",
    "    for i in range(1, len(text_list)):\n",
    "        current_text = text_list[i]\n",
    "        previous_text = merged_text\n",
    "        \n",
    "        # 检测 current_text 是否完全包含在 previous_text 中\n",
    "        if current_text in previous_text:\n",
    "            continue  # 跳过当前文本块，因为它已经被包含在合并后的文本中\n",
    "\n",
    "        # 检测 previous_text 是否完全包含在 current_text 中\n",
    "        if previous_text in current_text:\n",
    "            merged_text = current_text  # 直接使用 current_text 替换合并后的文本\n",
    "            continue\n",
    "\n",
    "        # 检测 previous_text 结尾和 current_text 开头的重叠部分\n",
    "        overlap_length_1 = 0\n",
    "        for j in range(min(len(previous_text), len(current_text)), 0, -1):\n",
    "            if previous_text[-j:] == current_text[:j]:\n",
    "                overlap_length_1 = j\n",
    "                break\n",
    "\n",
    "        # 检测 current_text 结尾和 previous_text 开头的重叠部分\n",
    "        overlap_length_2 = 0\n",
    "        for j in range(min(len(previous_text), len(current_text)), 0, -1):\n",
    "            if current_text[-j:] == previous_text[:j]:\n",
    "                overlap_length_2 = j\n",
    "                break\n",
    "\n",
    "        # 选择重叠部分较长的情况进行合并\n",
    "        if overlap_length_1 > overlap_length_2:\n",
    "            # previous_text 结尾和 current_text 开头重叠\n",
    "            merged_text += f\"\\n{current_text[overlap_length_1:]}\"\n",
    "        elif overlap_length_2 > overlap_length_1:\n",
    "            # current_text 结尾和 previous_text 开头重叠\n",
    "            merged_text = current_text + f\"\\n{merged_text[overlap_length_2:]}\"\n",
    "        else:\n",
    "            # 没有找到重叠部分，或者重叠部分长度相等\n",
    "            merged_text += f\"\\n{current_text}\"\n",
    "\n",
    "    return merged_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b55ce1d-c8a6-4692-a674-2668d882e63a",
   "metadata": {},
   "source": [
    "### Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "99bb0b73-86c8-43e0-a44f-28d28750659a",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-11-15T15:40:29.070031Z",
     "iopub.status.busy": "2024-11-15T15:40:29.069643Z",
     "iopub.status.idle": "2024-11-15T15:40:29.082746Z",
     "shell.execute_reply": "2024-11-15T15:40:29.082123Z",
     "shell.execute_reply.started": "2024-11-15T15:40:29.070005Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pruning(query, text):\n",
    "    # 分割句子\n",
    "    sentences = text.split('。')\n",
    "    # 去除空字符串，如果有的话\n",
    "    sentences = [s + '。' for s in sentences if s]  # 这里加上句号是为了保持输出格式的一致性\n",
    "    \n",
    "    subsequences = []\n",
    "    \n",
    "    # 生成所有连续子序列\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(i + 1, len(sentences) + 1):\n",
    "            # 将连续的句子合并成一个字符串\n",
    "            subsequence = ''.join(sentences[i:j]).strip()\n",
    "            subsequences.append(subsequence)\n",
    "            \n",
    "    sentences2 = text.split('\\n\\n')\n",
    "    # 去除空字符串，如果有的话\n",
    "    sentences2 = [s + '\\n\\n' for s in sentences2 if s]  # 这里加上句号是为了保持输出格式的一致性\n",
    "    \n",
    "    # 生成所有连续子序列\n",
    "    for i in range(len(sentences2)):\n",
    "        for j in range(i + 1, len(sentences2) + 1):\n",
    "            # 将连续的句子合并成一个字符串\n",
    "            subsequence = ''.join(sentences2[i:j]).strip()\n",
    "            subsequences.append(subsequence)\n",
    "    \n",
    "    #print(subsequences)\n",
    "    q = QueryBundle(query)\n",
    "    nodes = []\n",
    "    for i in subsequences:\n",
    "        n = NodeWithScore(node=TextNode(text=i))\n",
    "        nodes.append(n)\n",
    "    \n",
    "    reranked_nodes = reranker.postprocess_nodes(nodes, q)\n",
    "    \n",
    "    # for n in reranked_nodes:\n",
    "    #     print(n.text, n.score)\n",
    "    return reranked_nodes[0].text, reranked_nodes[0].score\n",
    "\n",
    "def pruning2(query, text):\n",
    "    # 分割句子\n",
    "    sentences = text.split('\\n\\n')\n",
    "    # 去除空字符串，如果有的话\n",
    "    sentences = [s + '\\n\\n' for s in sentences if s]  # 这里加上句号是为了保持输出格式的一致性\n",
    "    \n",
    "    subsequences = []\n",
    "    # 生成所有连续子序列\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(i + 1, len(sentences) + 1):\n",
    "            # 将连续的句子合并成一个字符串\n",
    "            subsequence = ''.join(sentences[i:j]).strip()\n",
    "            subsequences.append(subsequence)\n",
    "            \n",
    "    #print(subsequences)\n",
    "    q = QueryBundle(query)\n",
    "    nodes = []\n",
    "    for i in subsequences:\n",
    "        n = NodeWithScore(node=TextNode(text=i))\n",
    "        nodes.append(n)\n",
    "    \n",
    "    reranked_nodes = reranker.postprocess_nodes(nodes, q)\n",
    "    \n",
    "    result = []\n",
    "    seen_texts = []  # 创建一个集合来存储已见过的文本\n",
    "    max_score = reranked_nodes[0].score\n",
    "    final_text = reranked_nodes[0].text\n",
    "    scores = []\n",
    "    for n in reranked_nodes:\n",
    "        if n.score > max_score - 0.05 and n.text not in result and (not n.text.startswith('#')) and len(n.text)>40:\n",
    "            result.append(n.text)\n",
    "            seen_texts.append(n)  # 将新添加的文本加入到集合中\n",
    "            if len(n.text) < len(final_text):\n",
    "                final_text = n.text\n",
    "                \n",
    "            #print(n.text, n.score)\n",
    "    # for n in reranked_nodes:\n",
    "    #     print(n.text, n.score)\n",
    "    # result = result\n",
    "    # text = '\\n\\n'.join(result)\n",
    "    return final_text, reranked_nodes[0].score #reranked_nodes[0].text, reranked_nodes[0].score\n",
    "\n",
    "def pruning3(query, text):\n",
    "    \n",
    "    sentences = text.split('。')\n",
    "    # 去除空字符串，如果有的话\n",
    "    sentences = [s + '。' for s in sentences if s]  # 这里加上句号是为了保持输出格式的一致性\n",
    "    \n",
    "    q = QueryBundle(query)\n",
    "    nodes = []\n",
    "    for i in sentences:\n",
    "        n = NodeWithScore(node=TextNode(text=i))\n",
    "        nodes.append(n)\n",
    "    \n",
    "    reranked_nodes = reranker.postprocess_nodes(nodes, q)\n",
    "    \n",
    "    a = []\n",
    "    for node in reranked_nodes:\n",
    "        if node.score > 0.99:\n",
    "            \n",
    "            a.append(node.text)\n",
    "            \n",
    "    if len(a)>1:\n",
    "        return ''.join(a), reranked_nodes[0].score\n",
    "\n",
    "    return pruning2(query, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7374098-428c-4e71-a10d-414a51802f25",
   "metadata": {},
   "source": [
    "### Processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d9ab7fee-18f6-4163-b003-fad74d378929",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-11-15T15:40:33.249342Z",
     "iopub.status.busy": "2024-11-15T15:40:33.248679Z",
     "iopub.status.idle": "2024-11-15T15:47:58.187129Z",
     "shell.execute_reply": "2024-11-15T15:47:58.186527Z",
     "shell.execute_reply.started": "2024-11-15T15:40:33.249314Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "1it [00:03,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根据2023年年度报告摘要，中国联通在中央企业品牌建设能力对标中位于通信行业第几名？\n",
      "0.9996056\n",
      "2023年，中国联通坚持人民邮电为人民，聚焦高质量发展首要任务，不断深化业务融合、市场融通、能力融智，公司生产经营稳中有进，业务收入保持较快增长，用户规模创近年来新高，品牌影响力持续扩大，中国联通在中央企业品牌建设能力对标中，央企排名第四、通信行业位列第一。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "2it [00:12,  6.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "帮我查一下，2023年联通5G中频基站的数量和新增量分别是多少？\n",
      "0.9866736\n",
      "坚持贯彻新发展理念，践行5G高质量发展。5G中频基站新增22万站，4G共享基站超过200万站。与友商在新疆启动全球首个5G异网漫游试商用，进一步提升边远地区网络资源使用效益；努力推动传输线路、管道光缆等基础设施的共建共享共维取得积极成效。以实际行动响应“双碳”政策，2023年节省运营开支约人民币390亿元，减少碳排放约1,150万吨，累计节省资本开支约人民币3,400亿元。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "3it [00:18,  6.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查询2023年年度报告摘要，告诉我联通算网数智业务发展能力的增强主要体现在哪几个方面。\n",
      "0.99764\n",
      "算网数智业务发展能力不断增强，一是在算力服务上，联通云云池资源快速增长，完成“一市一池”布局。在政务云、央国企上云等重点领域，新增私有云标杆项目100+个。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "4it [00:24,  6.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023年中国联通通信业务的用户规模达到了多少？\n",
      "0.99361825\n",
      "公司不断深化业务融合、市场融通、能力融智，新融合积厚成势，移动、固网宽带及专线业务规模价值稳健增长，个人及家庭创新增值业务快速突破。2023年联网通信业务实现收入2,446亿元，包含移动联网、宽带联网、电视联网、专线联网、通信业务及信息业务六大板块，收入规模达到两类主营业务总收入的四分之三；联接规模进一步扩大，联网通信用户规模突破10亿，较2022年底增长约1.4亿。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "5it [00:29,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023年前三季度中国联通移动用户规模情况如何？\n",
      "0.9940433\n",
      "2023年首三季度，公司营业收入达到人民币2,816.9亿元，比去年同期提升6.7%，主营业务收入达到人民币2,525.2亿元，比去年同期提升5.4%。归属于母公司净利润为人民币75.8亿元，比去年同期提升10.9%。收入结构持续优化，产业互联网业务实现收入人民币606.9亿元，占主营业务收入比达到24.0%，比去年同期提升1.8pp。EBITDA1为人民币786.7亿元，比去年同期提升2.7%。\n",
      "\n",
      "面对数字经济与实体经济深度融合带来的深刻变革和广阔空间，公司坚定不移推进创新转型，积极储备核心能力，超前布局重点领域，数字化产品和解决方案核心竞争力不断增强。\n",
      "\n",
      "公司坚持量质构效协调发展，不断深化业务融合、市场融通、能力融智，基础业务稳盘托底效应更加突出。移动用户规模达到3.32亿户，移动用户净增达到967万户，推动公司移动主营业务收入实现人民币1,307.0亿元，同比增长2.6%，移动ARPU达到44.3元；固网宽带用户达到1.11亿户，年内净增用户突破783万户。宽带接入及应用收入同比增长8.7%达到人民币460.4亿元，宽带综合ARPU2达到47.9元，用户价值稳中有升。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "6it [00:37,  6.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023年前三季度中国联通新获专利数量同比提升了多少？\n",
      "0.9986588\n",
      "公司着力加强新型数字信息基础设施建设，加快5G、宽带、政企、算力网络建设，为经济社会发展筑牢数字底座，支撑企业高质量发展的网络基础更加坚实。积极拥抱数字经济的广阔蓝海，不断提高科技创新力，持续加大研发投入力度，年内授权专利数量达到1,888个，同比提升38.2%，战略性新兴产业布局进一步加快，扎实推进科技自立自强取得新成效。\n",
      "\n",
      "2023年是中国联通向数字科技创新企业转型升级的攻坚之年。公司将继续坚定践行网络强国、数字中国、智慧社会战略部署，着力提高企业核心竞争力和增强核心功能，为股东、客户及社会创造更大价值，以中国联通的高质量发展为中国式现代化作出新的更大贡献。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "7it [00:43,  6.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "告诉我2023年第一季度联通“大计算”业务的开展情况。\n",
      "0.9873016\n",
      "“大计算”方面，积极落实东数西算战略，不断深化“5+4+31+X”新型数据中心体系，IDC机架\n",
      "\n",
      "#中国联合网络通信股份有限公司\n",
      "\n",
      "#2023年第一季度报告\n",
      "\n",
      "数达到37.2万架。持续丰富云资源储备，优化一市一池算力布局，全面满足客户全场景上云需求，支撑数字政府、智慧城市建设，积极参与央企数字化转型，“联通云”保持良好增长，实现收入人民币127.9亿元，较去年同期提升40%。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "8it [00:49,  6.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023年第一季度中国联通大数据收入较去年同期提升了多少？\n",
      "0.9989637\n",
      "“大计算”方面，积极落实东数西算战略，不断深化“5+4+31+X”新型数据中心体系，IDC机架\n",
      "\n",
      "#中国联合网络通信股份有限公司\n",
      "\n",
      "#2023年第一季度报告\n",
      "\n",
      "数达到37.2万架。持续丰富云资源储备，优化一市一池算力布局，全面满足客户全场景上云需求，支撑数字政府、智慧城市建设，积极参与央企数字化转型，“联通云”保持良好增长，实现收入人民币127.9亿元，较去年同期提升40%。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "9it [00:56,  6.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国联通2023年第一季度报告指出，公司正在从传统管道运营商向什么类型的企业转型升级？\n",
      "0.9996642\n",
      "中国联通正在从传统管道运营商向数字科技领军企业加快跃迁升级。2023年，公司将继续坚定践行网络强国、数字中国、智慧社会战略部署，深入实施“1+9+3”战略规划体系，以新气象新作为推动高质量发展取得新成效，以数字化网络化智能化助力中国式现代化加速向前，为股东、客户及社会创造更大价值。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "10it [01:08,  8.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "告诉我中国联通2023年年度股东大会现场会议于何时召开？\n",
      "0.9986675\n",
      "现场会议地点：北京市西城区金融大街21号中国联通大厦\n",
      "\n",
      "网络投票平台：上海证券交易所股东大会网络投票系统\n",
      "\n",
      "#四、会议参会对象:\n",
      "\n",
      "(一)股权登记日（2024年5月20日）收市后在中国证券登记结算有限责任公司上海分公司登记在册的公司股东有权出席股东大会，并可以以书面形式委托代理人出席会议和参加表决。该代理人不必是公司股东；\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "11it [01:14,  7.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023年度董事会报告中指出，联通在原创技术策源地建设上取得了哪些成果？\n",
      "0.99542075\n",
      "推进高水平科技自立自强。董事会高度关注公司科技创新能力建设，在研究公司全年预算及投资计划议题过程中，持续加大科技创新投入强度。2023年，公司加快原创技术策源地建设，科技创新布局更加清晰，组建人工智能创新中心，主导国际标准立项40项，联通数科、中讯设计院入选国资委“创建世界一流和专精特新示范企业”，智网科技、智慧足迹入选国家级专精特新“小巨人”企业，“5G共建共享SA建设工程”获得国家优质工程金奖。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "12it [01:23,  8.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在2023年度，中国联通监事会召开了几次会议？\n",
      "0.9981184\n",
      "报告期内，监事会共召开5次会议，所有监事均出席了会议，所有审议事项均获通过。具体如下：\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "13it [01:30,  7.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查一下年度报告，告诉我2021年联通大数据业务收入和市场份额的情况。\n",
      "0.98301214\n",
      "2021年“联通云”收入人民币163亿元。\n",
      "\n",
      "大数据方面，升级数据应用服务、数据技术服务、数据安全服务、AI、区块链服务能力，聚焦政务、金融、文旅、交通等行业，优化政务大数据、金融大数据、文旅大数据、行业AI产品及“行业+区块链”产品体系；持续服务疫情防控与复工复产；支撑8项区块链试点成功入选国家区块链创新应用试点。2021年大数据收入达人民币26亿元，同比增长48.7%，市场份额连续三年保持电信运营商首位。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "14it [01:34,  6.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国联通2021年归属于上市公司股东的净利润相比上年的增长百分比是多少？\n",
      "0.9003919\n",
      "9|\n",
      "|归属于上市公司股东的净利润|4,033,310,444|3,344,217,849|20.6|\n",
      "|归属于上市公司股东的扣除非经常性损益的净利润|4,174,454,209|3,541,449,257|17.9|\n",
      "|经营活动产生的现金流量净额|51,415,346,436|48,636,520,694|5.7|\n",
      "|加权平均净资产收益率（%）|2.68|2.26|增加0.42个百分点|\n",
      "|基本每股收益（元／股）|0.131|0.108|21.3|\n",
      "|稀释每股收益（元／股）|0.131|0.108|21.3|\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "15it [01:40,  6.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021年上半年联通履行了哪些社会责任？\n",
      "0.33680895\n",
      "#匠心打造精品网络，促进业务有效益快速发展\n",
      "\n",
      "公司以客户感知为中心，聚焦围绕京津冀、长三角、粤港澳大湾区等重点城市群、重点城市、口碑场景，全力打造高品质网络，提升网络服务支撑能力。聚焦保障精品5G网、精品千兆光网建设，以及IDC、云计算、大数据、物联网等创新投资；持续完善政企精品网和商业楼宇覆盖；推动网络技术创新，构建CUBE-NET3.0网络体系，加快网络创新产品研发；推进网络AI助力智慧运营体系化，全国推广智能运维机器人；把握网络云化、NFV、SDN化、智能化契机，深化网络能力、网络资源、网络数据集中运营，积极推动自主研发4G/5G协同智能节能管理。上半年，公司5G网络质量满意度行\n",
      "\n",
      "#中国联合网络通信股份有限公司\n",
      "\n",
      "#2021年半年度报告摘要\n",
      "\n",
      "业排名第一，全国城市区域全面具备千兆宽带能力，网络高质量发展成效显著。\n",
      "\n",
      "#（四）混合所有制改革\n",
      "\n",
      "持续释放改革红利，发展动力进一步增强\n",
      "\n",
      "公司持续拓展与战略投资者的合作维度、广度和深度，推进合资公司产品研发迭代，聚焦多个协同优势领域强强联合、优势互补，增强创新发展新动能。迭代优化划小改革，强化数字赋能，完善考核激励，提升组织效能，激发微观主体活力。分层分类加快推动下属公司市场化改革，做强做优核心平台业务，智网科技完成股份制改革并拟分拆上市，加快发展和提升价值。优化科技创新管理模式，加快关键核心技术攻关，常态化引入数字化创新人才，加大力度推动科技创新，不断增强自主创新能力和发展动力。\n",
      "\n",
      "#全力加快数字化转型，着力打造不一样的中国联通\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "16it [01:45,  6.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022年中国联通股东大会资料指出公司在哪些方面承担了社会责任？\n",
      "0.9753434\n",
      "一年来，中国联通坚决贯彻落实党中央、国务院决策部署，勇担政治责任、经济责任、社会责任和科技创新责任，发展基础更加稳固，关系企业核心竞争力、可持续发展能力的关键基础和有利条件实现显著改善和强化，面向数字经济新需求的网络、产品和服务供给水平不断提升，坚决锻造科技创新核心优势，构建“一个联通、一体化能力聚合、一体化运营服务”的特色能力体系，实现新战略全面实施第一年的良好开局。公司荣获资本市场多项嘉奖，连续第二年荣获中国上市公司协会“董事会办公室最佳实践案例”及“业绩说明会最佳实践案例”评价，信息披露连续多年获上交所最高等级评价。\n",
      "\n",
      "现将董事会2022年主要工作情况报告如下：\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "17it [01:51,  6.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国联通第七届监事会第九次会议的审议内容是什么？\n",
      "0.9884465\n",
      "|会议届次|召开时间|会议地点及方式|审议事项（全部表决通过）|\n",
      "|---|---|---|---|\n",
      "|第七届监事会第十次会议|2023年1月18日|北京现场方式|1.关于相关会计估计变更事项的议案2.关于2022年年度报告的议案3.关于2022年度财务决算报告的议案4.关于2022年度计提资产减值准备及核销资产损失的议案5.关于日常关联交易的议案6.关于联通集团财务有限公司签订《金融服务协议》暨关联交易的议案|\n",
      "|第七届监事会第十一次会议|2023年3月8日|北京现场方式|1.关于2022年度末期利润分配的议案2.关于联通集团财务有限公司向中国联合网络通信有限公司提供非融资性担保业务的议案3.关于公司对外担保的专项说明的议案4.关于续聘会计师事务所的议案5.关于公司2022年度内部控制评价报告的议案6.关于公司首期限制性股票激励计划预留授予股票第三个解锁期解锁的议案|\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "18it [01:56,  5.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国联通董事会及各专门委员会在2022年及2023年分别召开了多少次会议？\n",
      "0.9964051\n",
      "董事会持续完善治理机制，推进合法规范运作，提升科学决策能力，为企业持续稳健发展提供制度保障。2023年，董事会及各专门委员会召开会议32次，审议议案58项，议案均获全票审议通过，无否决、缓议、弃权或设置限制性执行条件的情况。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "19it [02:01,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022年中国联通的“大联接”用户规模累计有多少？\n",
      "0.99918956\n",
      "着力打好“大联接、大计算、大数据、大应用、大安全”的“组合拳”，坚定挺进数字经济新蓝海。大联接方面，2022年全年，“大联接”用户规模达到8.6亿户，其中移网用户超过3.2亿户，5G套餐用户突破2亿户；宽带用户超过1亿户，千兆宽带用户渗透率达到15%；物联网连接数达到3.9亿户，率先实现“物超人”，物联网收入同比增长42%，增速大幅领先行业。大计算方面，推出联通云7.0版本，进一步强化“安全数智云”的品牌形象。其中公有云市场增速第一，打造了服务国家部委及地方政府的国家药监云、辽宁省政务云、广州市政务云和服务大型电商平台的茅台云等标杆产品，支撑了中国电建。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "20it [02:05,  5.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "联通在2022年和2020年的移动出账用户分别有多少？\n",
      "0.98607177\n",
      "2020年面对错综复杂的内外部环境和新冠疫情等挑战，中国联通始终坚持聚焦创新合作战略，积极推动全面数字化转型，坚持差异化经营，以5G业务为引领，强化融合经营，提升用户发展效能，积极推进高质量可持续发展。截至2020年底，本公司拥有移动出账用户约30,581.1万，固网宽带用户约8,609.5万，本地电话用户约4,733.9万。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "21it [02:16,  6.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022年年中和年底，联通数据中心IDC机架规模分别达到了多少？\n",
      "0.94802415\n",
      "数据中心梯次布局。三大电信运营企业均已明确多级数据中心/云资源梯次布局，并逐年加大投资倾斜，数据中心能力覆盖国家“东数西算”全部枢纽节点，算力规模保持高速增长。截止2023年6月末，中国电信对外IDC机架规模达到53.4万架，算力总规模达到#电信业发展白皮书——新时代高质量发展探索（2023年）\n",
      "\n",
      "#8.4EFLOPS\n",
      "\n",
      "中国移动对外可用IDC机架达到47.8万架，累计投产算力服务器超80.4万台，算力总规模9.4EFLOPS；中国联通IDC机架规模超过38万架，数据实时处理能力达到万亿级31。在骨干网络建设方面，电信运营企业大力推进400G高速光纤网络试点建设，打造“东数西算”大动脉，大幅降低网内平均时延，形成多级算力时延圈，推动实现“网络无所不达、算力无所不在、智能无所不及”。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "22it [02:21,  6.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022年第一季度期末，中国联通普通股股东总数是多少？\n",
      "0.9432812\n",
      "455|29,770,946,686|29,099,108,863|#中国联合网络通信股份有限公司\n",
      "\n",
      "#2022年年度报告\n",
      "\n",
      "#季度数据与已披露定期报告数据差异说明\n",
      "\n",
      "□适用√不适用\n",
      "\n",
      "#4股东情况\n",
      "\n",
      "#4.1报告期末及年报披露前一个月末的普通股股东总数、表决权恢复的优先股股东总数和持有特别表决权股份的股东总数及前10名股东情况\n",
      "\n",
      "|||单位:股|\n",
      "|---|---|---|\n",
      "|截至报告期末普通股股东总数（户）||553,063|\n",
      "|年度报告披露日前上一月末的普通股股东总数（户）||566,735|\n",
      "|截至报告期末表决权恢复的优先股股东总数（户）||不适用|\n",
      "|年度报告披露日前上一月末表决权恢复的优先股股东总数（户）||不适用|\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "23it [02:26,  5.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查询2020年中国联通证券事务代表的姓名。\n",
      "0.9571239\n",
      "|上海证券交易所|中国联通|600050|\n",
      "\n",
      "#联系人和联系方式\n",
      "\n",
      "||董事会秘书|证券事务代表|\n",
      "|---|---|---|\n",
      "|姓名|李玉焯|雷晓旭|\n",
      "|办公地址|北京市西城区金融大街21号|北京市西城区金融大街21号|\n",
      "|电话|010-66259179|010-66259179|\n",
      "|电子信箱|dongmi@chinaunicom.cn|ir@chinaunicom.\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "24it [02:32,  6.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020年上半年，联通哪家子公司被评为“亚洲最佳企业管治公司”？\n",
      "0.99811697\n",
      "公司持续完善公司治理机制，提升执行力，强化风险管理和内部控制，为企业持续健康发展提供坚实保障。今年以来，公司荣获多项嘉许，其中包括本公司控股子公司-联通红筹公司获《金融亚洲》Finance（Asia）评选为“亚洲最佳企业管治公司”，获《机构投资者》连续第五年评选为“亚洲最受尊崇电信企业”第一名，并于《福布斯全球上市公司2000强》中排名第286位。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "25it [02:36,  5.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020年前三季度，联通网间结算成本下降的主要原因是什么？\n",
      "0.99934596\n",
      "2020年首三季度, 网间结算成本比去年同期下降9.6%, 主要受网间话务量下滑影响.\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "26it [02:42,  5.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019年上半年中国联通的整体业绩情况怎么样？\n",
      "0.98908454\n",
      "下半年，公司将直面挑战，积极践行新发展理念，深化实施聚焦创新合作战略，纵深推进混合所有制改革，奋力开创“五新”联通建设和高质量发展的新局面。公司将坚定不移的深化互联网化运营，持续提升客户感知、运营效率和差异化竞争力，努力稳定基础业务，规模提升创新业务，扎实推进5G业务创新，驱动整体业务企稳增长。坚持聚焦战略，积极谋求5G网络共建共享，完善4G网络，简化优化网络结构，精准高效建设“5G+4G”精品网，为业务发展提供坚实保障。持续深化与战略投资者的业务和资本合作，纵深推进各领域的创新变革，推进降本增效，强化基础管理和风险管控，不断提升企业内生活力和运营效益，为股东创造更大价值。\n",
      "\n",
      "#第二部分：财务情况讨论与分析\n",
      "\n",
      "#1．概述\n",
      "\n",
      "2019年上半年公司实现营业收入人民币1,449.5亿元，同比下降2.8%，主营业务收入实现人民币1,329.6亿元，同比下降1.1%。归属于母公司净利润实现人民币30.2亿元，同比增长16.8%。2019年上半年公司经营活动现金流量净额为人民币435.9亿元，资本开支为人民币220.5亿元。截至2019年6月30日，公司资产负债率为43.6%。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "27it [02:48,  5.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国信通院预计2023年5G技术对经济增长带来多大程度的影响？\n",
      "0.99786144\n",
      "9间接带动经济总产出和经济增加值的增长率测算方法同上。#中国5G发展和经济社会影响白皮书（2023年）\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "28it [02:52,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "有效提升通信网络覆盖深度广度的措施有哪些？\n",
      "0.9829549\n",
      "#（一）有效提升通信网络覆盖深度广度\n",
      "\n",
      "加快推进“双千兆”网络建设。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "29it [02:55,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "工业互联网是推进产业结构优化升级的重要抓手包括哪些方面？\n",
      "0.9990109\n",
      "如河南航天液压气动技术有限公司基于航天云网INDICS平台实现了与总体设计部、总装厂所的协同研发与工艺设计，研发周期缩短35%、资源利用率提升30%、生产效率提高40%。\n",
      "\n",
      "#（四）工业互联网是推进产业结构优化升级的重要抓手\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "30it [03:03,  5.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023年全球人工智能市场收入以及同比增长分别是多少？\n",
      "0.9996026\n",
      "2023年全球人工智能产业规模高速增长，预计未来增速将逐渐放缓。2023年全球人工智能市场收入达5132亿美元，同比增长20.7%，到2026年市场规模将达8941亿美元。其中，软件在市场中持续占据主导地位，其市场份额占比近九成，2023年市场规模为4488亿美元，同比增长20.4%。区域来看，美洲地区市场规模最大，2023年将达2886亿美元，占AI软件市场的64.3%；亚太地区市场规模较小，2023年为550亿美元。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "31it [03:36, 13.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全球数字经济发展愿景主要有哪几点？\n",
      "0.9915793\n",
      ".30\n",
      "\n",
      "#四、全球数字经济发展愿景\n",
      "\n",
      ".39\n",
      "\n",
      "#（一）协力强化数字技术创新性前瞻性战略性布局\n",
      "\n",
      ".39\n",
      "\n",
      "#（二）强化提升新型基础设施建设水平和服务能级\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "32it [03:40, 10.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "内蒙古联通绍天纲已获得国家专利授权和软件著作权各有多少项？\n",
      "0.69585246\n",
      "经过70年的发展，中讯院已为国家培养塑造了一大批行业领军人才和业界楷模，先后承担了几十项“中国第一”的通信高新技术工程设计，完成了中国通信骨干网50%以上的总体规划和设计任务，累计获得国家级科学技术进步奖3项，省部级科技奖352项，国家优秀设计奖90项，国家级设计咨询奖项153项，省部咨询设计奖1205项。发布国家标准37项，行业标准212项。获得专利授权319项，软件著作权161项，同行业中名列前茅。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "33it [03:50, 10.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国联通河南分公司根据太昊陵伏羲文化旅游区的特定信息化需求制定了一套怎样的解决方案并实现了哪些功能？\n",
      "0.99997413\n",
      "2022年，中国联通河南分公司根据太昊陵伏羲文化旅游区的特定信息化需求，结合国家5A级景区的创建要求，制定了一套全方位的解决方案。这一方案包括部署一张高效的通信网络和一套综合服务管理平台，实现了景区综合管控、客流监测、环境监测、游船调度、门禁票务、智慧停车和应急指挥等多种功能，极大地提升了景区的运营效率和游客体验。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "34it [03:54,  8.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "面对中国联通“百年传承 三十向新”的关键历史节点，中国联通董事长陈忠提出了哪三点重要倡议？\n",
      "0.99979466\n",
      "在中国联通“百年传承三十向新”的关键节点，陈忠岳针对如何聚焦网络向新、算网向新提出三点倡议。一是与合作伙伴共同推进算网向新，加速打造算力智联网策源地；二是与合作伙伴共同推进算网服务，加速打造算网数智供给新高地；三是与合作伙伴共同推进算网生态，加速打造算网产业资源集聚地。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "35it [04:00,  7.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国联通三农AI联合实验室组建的宗旨是什么?\n",
      "0.99937564\n",
      "由乡村振兴（重庆）数字产业研究院牵头，重庆市农业科学院、上海交通大学重庆人工智能研究院和重庆供销电商共同组建。该实验室的宗旨构建“三农”领域的开放共享、产学研用深度融合的智慧平台。实验室将通过聚集多方资源，打造一个涵盖农业生产、农村治理和农民生活的智能化服务体系。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "36it [04:08,  7.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "联通元景大模型为了面向人工智能赋能实体经济的需要提供了哪些体系化产品?\n",
      "0.9997578\n",
      "为了满足面向人工智能赋能实体经济的需要，提供从基础大模型到大模型平台再到行业大模型的体系化产品。具体来说，它包括1套基础大模型、1个大模型MaaS平台和多种行业大模型。这套基础大模型不仅涵盖了多参数版本的语言大模型和多模态大模型，而且在同参数级别下，其性能达到了业\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "37it [04:12,  6.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国联通河北沧州公司纸房头营业部张玉，2012年以来荣获过哪些荣耀?\n",
      "0.9363264\n",
      "问渠那得清如许，为有源头活水来。张玉抚摸着中国联通好员工等厚厚的荣誉证书，心里深深明白，自己这些成绩的取得，离不开优质服务这一源头活水。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "38it [04:16,  5.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“联通小象”什么时候正式成为广西联通的服务品牌？\n",
      "0.9993741\n",
      "2021年5月18日, “联通小象”正式成为广西联通服务品牌, 自创建以来共策划组织近百场社会公益品牌活动, 不断树立“行业新风”, 传播社会正能量, 取得了良好的社会效应, 持续提升企业服务品牌宣传效果, 曾荣获“2019-2020年度桂林市青年文明号集体”、2019年中国联通“党建工作示范点”、2021年中国联通“五四红旗团支部”、2022年中国联通“青年文明号”等荣誉.\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "39it [04:20,  5.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "山东联通如何帮助山东鑫博奥公司实现了数字化转型？\n",
      "0.9477388\n",
      "今年1月，工业和信息化部等11部门联合发布通知，开展“信号升格”专项行动。山东联通积极推进“信号升格”专项行动，加强重点场景4G/5G网络深度覆盖，全力支撑重点行业数字化转型需要。\n",
      "\n",
      "从建设智能高效的智慧工厂，到携手各级政府推进政务服务智慧化改革，再到助力社区实现智能化管理和乡村全面数智化转型，联通公司致力于整合先进的信息技术，赋能各领域实现全方位、深层次的数字化变革，携手各方共同迈向数字经济的广阔未来。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "40it [04:24,  4.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "美的空调泰国工厂占地面积是多少?\n",
      "0.7695296\n",
      "具体来说，项目通过建设1套BBU、7套RRU及宏站，实现了对16万平方米工厂的全面定制化专网覆盖。同时，团队部署了超过10个5G应用，从而实现了工厂内各个生产环节的高效互联和智能化管理。为了确保项目的顺利推进，中泰两国基于5G技术形成了一套共建共维、融合运营的跨国协作机制。这一机制不仅有效提升了项目的实施效率，也为未来更多跨国5G项目的合作提供了宝贵经验。通过这一机制，中泰双方在5G技术的应用和推广上实现了深度合作，共同推动了5G技术在工业互联网领域的跨国应用。\n",
      "\n",
      "#开启5G技术出海向5G产业出海的新进程\n",
      "\n",
      "从终端来看，此次项目成功催生了100套5G终端出海，为中国5G行业终端和模组取得泰国入网许可证奠定了基础。这不仅是中国5G技术快速融入泰国无线网络领域的重要里程碑，也标志着中国5G设备在国际市场上的影响力进一步增强。通过此次项目的推动，中国5G技术在全球范围内的推广和应用迎来了新的契机。从平台来看，基于中国联通5G多园区专网方案，通过部署自主研发的5G专网自服务平台，项目实现了美的集团全球5G工厂的跨域互联和统一运营管理。这个平台不仅大幅降低了运维成本，还提升了运营效率，使得工厂能够更高效地进行生产管理和数据监控。通过这种创新的管理模式，美的集团不仅实现了生产设备的高效连接和实时监控，还显著提高了生产效率和产品质量，降低了生产成本，增强了市场竞争力。从应用来看，5G技术在美的空调泰国工厂的应用范围广泛，涵盖了从生产到管理的各个环节。百兆上行大带宽和毫秒级低时延的5G确定性专网，使能了5G智能集采、5G自动导引车、5GAI质检、5G机械臂控制等多项应用。这些应用的快速落地，不仅提升了工厂的生产效率，还提高了产品质量和生产灵活性。例如，5G智能集采系统可以实时监控和管理物料的采购和配送，确保生产线的连续性和高效性。5G自动导引车则提高了物料运输的效率和准确性，减少了人为操作的误差。5GAI质检系统通过高速数据传输和实时分析，能够快速准确地检测产品质量，降低了次品率。5G机械臂控制系统则实现了生产线的自动化和智能化，提高了生产效率和产品的一致性。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "41it [04:28,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“星罗”先进算力调度平台的主要功能是什么？\n",
      "0.9981742\n",
      "“星罗”先进算力调度平台拥有六大核心能力：\n",
      "\n",
      "-多元异构的算力融合调度能力让CPU/GPU/NPU等异构芯片得以统一纳管；\n",
      "-集群管理功能支持千卡及万卡规模集群的高效管理并快速解决多租户隔离和故障定位问题；\n",
      "-算力增效功能则能够显著提升训练场景下智算大规模集群的并行加速比及算力利用率；\n",
      "-算力调度能力则提供了多站点、多策略的灵活调度方案；\n",
      "-推理分发功能支持超200个骨干云池及边缘一体机的定点推送和一键多点分发；\n",
      "-而数据跨域优化则能够提升算存协同服务水平实现更高效的数据利用.\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "42it [04:32,  4.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国联通人工智能创新中心推出的元景2.0实现了哪四项能力升级？\n",
      "0.9998952\n",
      "\n",
      "\n",
      "#百年传承\n",
      "\n",
      "#三十向新\n",
      "\n",
      "#联通元景:更懂行业的大模型,产业升级的智能引擎\n",
      "\n",
      "朱常波介绍，自2024巴塞罗那世界移动通信大会上发布元景1.0以来，中国联通人工智能创新中心经过半年的技术钻研与应用实践，推出了元景2.0，实现了四项能力升级：基座能力升级、MaaS平台升级、安全能力升级和行业应用升级。这四大升级让元景2.0具备了更强引擎，形成了“更易定制、更懂行业、更加可信”的鲜明特色。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "43it [04:36,  4.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国联通智慧社区平台在内蒙古已累计接入小区和注册用户分别是多少?\n",
      "0.9998171\n",
      "目前, 在内蒙古, 中国联通智慧社区平台已累计接入小区2855个, 注册用户达60.5万人, 2024年计划再建设1000个智慧社区, 进一步拓展智慧社区服务的覆盖范围.\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "44it [04:41,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022年中国联通合作伙伴大会主旨演讲中提及了哪些5G+方面的已有成果？\n",
      "0.50909126\n",
      "一年来，中国联通坚决贯彻落实党中央、国务院决策部署，勇担政治责任、经济责任、社会责任和科技创新责任，发展基础更加稳固，关系企业核心竞争力、可持续发展能力的关键基础和有利条件实现显著改善和强化，面向数字经济新需求的网络、产品和服务供给水平不断提升，坚决锻造科技创新核心优势，构建“一个联通、一体化能力聚合、一体化运营服务”的特色能力体系，实现新战略全面实施第一年的良好开局。公司荣获资本市场多项嘉奖，连续第二年荣获中国上市公司协会“董事会办公室最佳实践案例”及“业绩说明会最佳实践案例”评价，信息披露连续多年获上交所最高等级评价。\n",
      "\n",
      "现将董事会2022年主要工作情况报告如下：\n",
      "\n",
      "#一、全力促发展，引领公司锻造新能力、迸发新活力、实现新突破\n",
      "\n",
      "2022年，董事会坚决践行新时代赋予的新使命，立足公司“数字信息基础设施运营服务国家队、网络强国数字中国智慧社会建设主力军、数字技术融合创新排头兵”新定位，全面推进“1+9+3”战略规划体系见行见效，打好大联接、大计算、大数据、大应用、大安全的“组合拳”，发挥“一个联通、一体化能力聚合、一体化运营服务”的特色优势，持续优化发展质量和结构。\n",
      "\n",
      "（一）坚决贯彻落实党中央、国务院决策部署。董事会自觉把公司发展放到党和国家大局中统筹推进，引领各项工作实现新的提升。贯彻落实网络强国、数字中国战略部署，坚定不移加强数字信息基础设施建设。坚\n",
      "\n",
      "#中国联合网络通信股份有限公司2022年年度股东大会会议资料\n",
      "\n",
      "持适度超前、以建带用、以用促建，持续深化5G共建共享，加快推进“双千兆”网络协同发展，全面承接“东数西算”工程，着力打造5G/4G、千兆宽带、算力、政企“四张精品网”，当好服务网络强国、数字中国建设的“铺路人”。与中国电信累计开通5G共享基站100万站，实现乡镇及以上区域5G网络连续覆盖。深化4G共建共享，累计开通4G共享基站110万站。通过5G/4G共建共享，双方累计节省投资超过2700亿元，节约运营成本每年超过300亿元，减少碳排放每年超1000万吨。加快推进900MHz低频打底网建设，快速新建17万基站，有力提升农村及边远地区的网络覆盖水平。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "45it [04:46,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "人工智能共享数据集AIData的发布日期是什么时候？\n",
      "0.8030475\n",
      "会上, 中国联通数字化部副总经理娄瑜正式发布了中国联通人工智能共享数据集AIData, 并与新华社、阿里云、腾讯云、百度智能云、库帕思、人民数据、南方电网、中国数字文化集团、中国航信、中国信通院等多个合作伙伴联合发起了“共建高质量数据集合作倡议”.\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "46it [04:49,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024中国联通合作伙伴大会算网生态大会上，中国工程院院士刘韵洁所作分享的主题是什么？\n",
      "0.9976046\n",
      "主旨演讲环节，中国工程院院士、紫金山实验室首席科学家刘韵洁作了题为《AI推动网络发展模式创新》的精彩分享。刘韵洁表示，把AI的解决目标和承载方式都设在网络内部，将充分赋能新一代信息基础设施，开创前所未有的可能性。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "47it [04:55,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国联通人工智能赋能全域数字化论坛上提到了哪些服务县域数字化的案例？\n",
      "0.9959902\n",
      "联通数字科技有限公司董事长李广聚在演讲中详细介绍了中国联通在县域数字化建设方面的“联通方案”。他指出，县域数字化转型过程中面临着诸多挑战，包括资金门槛、技术门槛和运营能力等方面的难题。为此，中国联通综合施策、因地制宜，推出了针对不同县域特点的解决方案。一方面，为那些经济发展较为薄弱的县域提供经济型数字化服务方案，帮助其实现基础数字化服务的覆盖；另一方面，为经济较为发达的县域提供产业型智慧县域方案，通过一整套数智工具箱，满足其在产城融合、特色产业、营商环境和乡村振兴等方面的多样化需求。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "48it [04:59,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“格物”设备管理平台支持哪些协议类型？\n",
      "0.9975508\n",
      "另外, 针对一些厂家的私有化协议, “格物”设备管理平台还支持通过泛协议SDK编写适配器, 让这些终端也能无缝上云, 同时这些适配器还可以作为可选插件进行复用.\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "49it [05:03,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "联通云如何保障企业的容器环境安全？\n",
      "0.99869055\n",
      "联通云重点关注容器全生命周期防护及持续的监控与分析, 以此保障企业的容器环境安全, 用户可通过低代码方式快速构建安全可视化大屏, 展现企业风险场景、分析风险原因, 使容器安全清晰可衡量.\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "50it [05:06,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国联通生态合作伙伴大会上，哪几位院士被聘请为中国联通大数据原创技术特聘院士专家？\n",
      "0.99983096\n",
      "中国联通特邀请中国科学院院士、北京大学教授梅宏院士, 中国科学院院士、清华大学教授王小云院士, 加拿大工程院院士、欧洲科学院院士、郑州大学学术副校长杨天若院士, 欧洲科学院院士、北京大学博雅讲席教授张大庆院士, 为中国联通大数据原创技术特聘院士专家.\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "51it [05:10,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元景2.0实现了哪四项能力的升级？\n",
      "0.99922454\n",
      "朱常波介绍, 自2024巴塞罗那世界移动通信大会上发布元景1.0以来, 中国联通人工智能创新中心经过半年的技术钻研与应用实践, 推出了元景2.0, 实现了四项能力升级：基座能力升级、MaaS平台升级、安全能力升级和行业应用升级.\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "52it [05:15,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元景2.0如何解决大模型进行RAG时“不会查、看不全、找不对”的问题？\n",
      "0.9996941\n",
      "朱常波介绍，大模型使用RAG就像人类首次使用搜索引擎一样，存在“不会查、看不全、找不对”的问题。为了解决因短问题和长文本块匹配度低而“不会查”，元景2.0创新性地使用了级联切分的方法，切出各种长度不同的文本块，实现召回率5%的提升；为解决表格特别是长表格，在建知识库时被从中间切开而“看不全”的问题，元景2.0创造性地使用了自适应的表格拆分和整合，自动补齐了表头和标题等信息，使表格问答的准确率提升了20个百分点；针对车牌号、故障码等字符串查询“找不对”的问题，元景2.0采用多路检索融合的方式，使回答准确率提升近20个百分点。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "53it [05:19,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "联通元景在辽宁12345省级平台的应用提升了哪些服务指标？\n",
      "0.978379\n",
      "现场还发布了中国联通和广大合作伙伴一起落地的百个优秀案例, 例如辽宁12345省级平台的全部坐席已全面接入政务热线大模型, 实现了工单填单时间缩短80%, 工单记录完整度提升30%, 内容推荐准确率提升35%.\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "54it [05:23,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "联数网和可信数据资源空间是什么？\n",
      "0.9969144\n",
      "总之, “联数网”与“可信数据资源空间”的建设, 是中国联通在大数据领域的一次重要布局和战略突破.\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "55it [05:34,  6.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国联通在响应绿色低碳战略过程中，与哪个企业合作探索了无线网络共建共享模式？\n",
      "0.99914086\n",
      "随着信息技术的飞速发展，5G网络的普及正以前所未有的速度改变着人们的生活。然而，这背后是巨大的能源消耗与环境压力。中国联通深刻认识到，在满足日益增长的通信需求的同时，实现节能减排、绿色发展是必由之路。为此，中国联通与中国电信携手合作，共同探索无线网络共建共享的新模式。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "56it [05:38,  5.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "郝鹏在文章中提到，国资委党委推动新时代国资央企的历史性成就集中体现在哪些方面？\n",
      "0.9995561\n",
      "国资委党委深入学习领会、坚决贯彻落实, 推动新时代国资央企通过革命性锻造、系统性重塑, 取得了历史性成就, 集中体现在取得重大政治成果、实践成果和制度成果上.\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "57it [05:43,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“2022年中国网信十大技术产品”榜首产品是什么？\n",
      "0.99925166\n",
      "其它入选“2022年中国网信十大技术产品”的依次是：\n",
      "\n",
      "#麒麟软件“星光麒麟”万物智联操作系统\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "58it [05:47,  4.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "央地携手合作落实成渝地区双城经济圈建设国家战略共建现代化新重庆座谈会的主持人是谁？\n",
      "0.96343076\n",
      "会上，重庆市委常委、常务副市长陈鸣波介绍了重庆经济社会发展和成渝地区双城经济圈建设情况，对深化重庆与央企合作提出建议。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "59it [05:53,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国联通与中国电信共建共享的具体成效是什么？\n",
      "0.91956544\n",
      "公司致力于打造覆盖完备、体验领先、能力领先、效能领先的5G网络，持续深入推动共建共享。继续与中国电信紧密合作，新开通5G基站18万站，双方累计开通5G基站87万站，室分覆盖能力显著增强，新开通5G室分基站9万站，双方累计开通5G室分基站20万站，建成全球规模最大的5G共建共享网络。与中国电信加快推进4G一张网，4G共享规模进一步突破，上半年新开通4G共享基站21万站，累计共享4G基站达到87万站。目前，4/5G共享基站已全部纳入区块链管理平台，网络协同效率大幅提升。通过4G/5G网络共建共享，公司积极助力“碳达峰、碳中和”双碳目标，累计减排二氧化碳超过600万吨，并累计为双方节省CAPEX人民币2,400亿元，每年节省OPEX人民币255亿元。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "60it [05:56,  4.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一体推进“三不腐”中的关键是什么？\n",
      "0.99643695\n",
      "\n",
      "\n",
      "#持续强化不能腐的刚性制度约束，不断完善防治腐败滋生蔓延的体制机制\n",
      "\n",
      "#在一体推进“三不腐”中\n",
      "\n",
      "不能腐是关键，侧重于扎紧笼子、堵塞漏洞。在一体推进“三不腐”中，不想腐是根本，解决的是世界观、人生观、价值观这个“总开关”问题。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "61it [06:00,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中星26号通信卫星的发射日期是哪一天？\n",
      "0.9941498\n",
      "据了解, 中星26号是目前国内接入容量最大的一颗通信卫星, 于2023年2月23日发射成功, 单星容量超过100Gbps, 可支持百万终端通信, 最大可提供450Mbps的通信能力.\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "62it [06:04,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“两增一控三提高”具体是指什么？\n",
      "0.6609626\n",
      "“一增”即确保利润总额增速高于全国GDP增速, 力争取得更好业绩, 增大国资央\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "63it [06:10,  4.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国联通大数据首席科学家范济安在人工智能赋能新型工业化论坛上发布了什么计划？\n",
      "0.9997925\n",
      "在此次论坛上，中国联通大数据首席科学家范济安重磅发布了“格物Unilink工业互联网平台AI+计划”。这一计划不仅代表着技术的重大突破，更象征着工业互联网发展模式的革新，为工业智能化升级开辟了新的道路。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "64it [06:14,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国联通成立于哪一年？\n",
      "0.9973519\n",
      "“三十向新”——1994年7月19日, 中国联合通信有限公司(简称中国联通)成立大会在北京钓鱼台国宾馆召开, 拉开了我国电信业体制改革的序幕.\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "65it [06:19,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国联通的创始人胡启立同志在原电子工业部担任了部长几年？\n",
      "0.99968827\n",
      "“饮水思源”。追忆中国联通历史，人们不能不想起中国联通的创始人、原电子工业部部长胡启立同志。他1993年至1998年他担任电子工业部部长期间，根据世界经济和技术革命的发展趋势，结合国情，高瞻远瞩地把信息化作为国家重大战略任务，主导推动了包括成立中国联通、实施三金(“金桥”、“金卡”、“金关”)工程、组织启动“909”超大规模集成电路工程等一系列重大项目的完成，大大推进了中国信息化进程，并成功实现电子工业由传统的单一制造业向现代电子信息产业的转型升级。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "66it [06:24,  4.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "截至2008年8月，中国的固化装机和移动电话数量分别是多少？\n",
      "0.9989968\n",
      "那之后, 中国的固化装机和移动电话数量成为世界第一就只是时间问题了!截至2008年8月, 全国共有固话用户3.54亿户, 移动电话用户6.16亿户, 两项均为世界第一.\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "67it [06:29,  4.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在上海联通沉浸式数字生活体验厅内，能体验哪些沉浸式数字生活？\n",
      "0.99977463\n",
      "随着5G赋能千行百业，数字化赋能不断深入，智慧营业厅的内容不断与时俱进。2023年5月，首条2000兆全屋光宽带在杭州联通延安路营业厅开通，现场可感受千兆速率翻倍带来极速体验。在上海联通沉浸式数字生活体验厅内，5G慢直播、数字人全息仓、裸眼3D等沉浸式数字生活体验应有尽有，仿若穿越时空、置身未来。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "68it [06:33,  4.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我国哪一年开始发放的4G牌照？\n",
      "0.9933634\n",
      "2013年, 我国开始发放4G牌照, 网络代际更迭迅速引爆了通信行业规模扩张和创新速度, 互联网随之进入到移动互联网时代.\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "69it [06:37,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009年3G牌照发放，中国联通拿到的牌照是什么？\n",
      "0.9991628\n",
      "2009年3G牌照发放，中国联通拿到了WCDMA牌照。2009年4月份，联通推出了全业务品牌“沃”，英文为“WO”。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "70it [06:41,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在2017年12月26日的北京冬奥会合作伙伴的签约仪式中，中国联通提出了什么主题理念？\n",
      "0.999979\n",
      "在2017年12月26日的北京冬奥会合作伙伴的签约仪式中, 中国联通提出了“智慧冬奥联通未来”的主题理念.\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "71it [06:45,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国联通为了向青少年和校园学生群体普及冬奥知识和文化，启动了什么活动？\n",
      "0.99952185\n",
      "中国联通还启动了“校园冬奥计划”，该活动时间将持续四年左右，旨在青少年和校园学生群体中进一步传播奥林匹克精神，普及冬奥知识和文化。“校园冬奥计划”的首个项目“中国联通校园冬梦大使”即日起至9月30日开启线上报名和投票，全国在校大学生均可参与，也可扫描活动相关海报二维码报名。招募通过学校推荐和投票综合考量，排名前2022名选手将获得“中国联通校园冬梦大使”称号，并有机会参加中国联通冬奥通信保障培训及实践活动，以实际行动为北京冬奥会贡献力量。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "72it [06:49,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我国什么时候开通的第一个大规模商用模拟移动通信系统？\n",
      "0.99913055\n",
      "1987年10月, 我国开通第一个大规模商用模拟移动通信系统, 时任邮电部部长杨泰芳拨通了中国第一个移动电话.\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "73it [06:52,  3.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "截至2024年5月底，中国联通“大联接”用户有多少？\n",
      "0.99891317\n",
      "截至2024年5月底，中国联通“大联接”用户超过10.5亿户。其中5G套餐用户2.7亿户，5G套餐渗透率超80%，物联网连接数达到5.5亿个。时光如梭，网络从1G到5G，连接从零到10亿，主动求变求新是中国联通始终不变的精神特质，每次新号码的推出都代表了新的技术飞跃，也代表了中国联通新一轮变革创新的开始。向新生长，中国联通永远在路上。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "74it [06:58,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "福建联通建成的“闽宁云”算力服务项目在何时登上央视新闻联播？\n",
      "0.9993679\n",
      "从2020年在省内率先建成“一城一池”的联通云能力，到2022年打造了高品级直连香港“一带一路”枢纽和宁夏“东数西算”中心的区域级智云中心——福州、厦门两大智云数据中心，福建联通发挥独有区位优势，携手宁夏联通建设了国家“东数西算”战略的“第一朵云”——闽宁云，为共建“一带一路”、“闽台两岸融合”“闽港合作”提供了全方位、更高质量的服务。2023年11月26日，福建联通和宁夏联通共同打造的“闽宁云”算力服务厦门金龙客车项目登上央视新闻联播，吸引了社会高度关注。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "75it [07:02,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024年福建联通的移动网人口覆盖率和行政村覆盖率分别是多少？\n",
      "0.9994344\n",
      "从1996年福建联通各地市GSM网陆续开通、2000年实现网络省内覆盖“县县通”，到2024年在全省开通3.8万个5G基站，移动网人口覆盖率达99.9%、行政村覆盖率达99.8%，福建联通实现5G网络覆盖所有地市城区、县城城区和乡镇镇区，并在全省高速公路、动车、高铁等重要交通干线着力打造优质网络，满足出行旅客对流畅上网的需求，实现全省第一条时速350公里的福厦高铁的5G信号全覆盖。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "76it [07:06,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024年青海联通的网络建设覆盖情况如何？\n",
      "0.9386294\n",
      "在广袤无垠的雪域高原上, 网络建设成本高、难度大、覆盖地域广, 伴随1G到5G的演进历程, 从建立第一个基站开始, 青海联通人发扬“高原背包客”的坚韧精神, 建成覆盖全省人口98.5%, 覆盖96.4%的行政村的精品移动网络, 无论是偏僻闭塞的小山村还是草原游牧人民都能通过网络阅尽大千世界, 为高原人民建起联接世界的网络, 筑起了一条横跨“数字鸿沟”的“信息公路”.\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "77it [07:11,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国联通成为北京2022年冬奥会和冬残奥会官方通信服务合作伙伴是在哪一天？\n",
      "0.99989307\n",
      "2017年12月26日, 中国联通正式成为北京2022年冬奥会和冬残奥会唯一官方通信服务合作伙伴, 成为“双奥之城”里的“双奥企业”.\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "78it [07:15,  4.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "截至2024年7月19日，中国联通全网智算算力达到了多少？\n",
      "0.9670405\n",
      "中国联通网络部副总经理赵静宜指出，中国联通聚焦网络强国、数字中国主责，拓展联网通信、算网数智主业，联合产业链合作伙伴，加快算网基础设施布局。为进一步促进算网生态发展，中国联通重点构筑以下三方面能力。一是打造算力新供给，着力成为生态发展的“筑基者”。打造上海、呼和万卡智算集群，全网智算算力超15EFLOPS，发布AICC、AICP、星罗调度平台等五大智算产品，提供涵盖国家“东数西算”枢纽、31省重点城市、超600边缘节点的AIDC基座。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "79it [07:20,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国联通在智慧城市和数字乡村平台建设方面有哪些应用？\n",
      "0.9955446\n",
      "在智慧城市与数字乡村建设方面，联通云同样展现出了非凡的能力。服务超1400个智慧城市的经历，让联通云深刻理解了城市治理的复杂性和多样性。从智慧交通到智慧医疗，从智慧环保到智慧教育，联通云以其全面的解决方案和高效的实施能力，为城市管理者提供了强大的技术支持。同时，在数字乡村服务上，联通云也未曾缺席，覆盖超25万行政村的数字乡村服务，让偏远的农村地区也能享受到数字化带来的便利与福祉。这不仅缩小了城乡之间的数字鸿沟，更为乡村振兴战略的深入实施提供了有力支撑。\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "80it [07:24,  5.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国联通累计开通的4G和5G共享基站分别有多少站？\n",
      "0.99922717\n",
      "中国联通携手中国电信累计开通5G共享基站百万站, 开通4G共享基站155万站, 特别是快速新增了17万站900MHz低频打底网, 现已拥有117万个5G基站, 占到全球30%以上.\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import time\n",
    "if __name__ == \"__main__\":\n",
    "    question_df = pd.read_csv(\"B_question.csv\")\n",
    "    question_df = question_df\n",
    "    sumbit_data = []\n",
    "    for index, row in tqdm(question_df.iterrows()):\n",
    "        ques_id = row['ques_id']\n",
    "        query = row['question']\n",
    "        start_time = time.time()\n",
    "        candidate_answer = Hybrid_retriever(query)\n",
    "        reranked_answer = get_rerank3(query, candidate_answer)\n",
    "        reranked_text = [n.text for n in reranked_answer]\n",
    "        merge_answer = get_merge(reranked_text)\n",
    "        final_answer, score = pruning3(query, merge_answer)\n",
    "        embedding = embed_model.get_text_embedding(final_answer)\n",
    "        \n",
    "        result = {\n",
    "            'ques_id': ques_id,\n",
    "            'question': query,\n",
    "            'answer': final_answer,\n",
    "            'embedding': embedding,\n",
    "            'score': score\n",
    "        }\n",
    "        print(query)\n",
    "        print(score)\n",
    "        print(final_answer)\n",
    "        print(\"========================\")\n",
    "        sumbit_data.append(result)\n",
    "    sumbit_df = pd.DataFrame(sumbit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "600d4326-1b97-447c-85ed-4d47575a5abe",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-11-15T15:48:03.512983Z",
     "iopub.status.busy": "2024-11-15T15:48:03.512587Z",
     "iopub.status.idle": "2024-11-15T15:48:03.702177Z",
     "shell.execute_reply": "2024-11-15T15:48:03.701571Z",
     "shell.execute_reply.started": "2024-11-15T15:48:03.512958Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# 获取当前时间并格式化\n",
    "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# 构建新的文件名\n",
    "filename = f\"./submit/submit_{current_time}.csv\"\n",
    "filename_s = f\"./submit/submit_{current_time}_score.csv\"\n",
    "\n",
    "# 将DataFrame保存为CSV文件\n",
    "sumbit_df.to_csv(filename_s, index=False, encoding='utf-8')\n",
    "\n",
    "sumbit_df = sumbit_df.drop(columns=['score'])\n",
    "sumbit_df.to_csv(filename, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "713ee5b6-4a26-4fc1-af24-687b8205c3f7",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-11-15T15:37:54.216635Z",
     "iopub.status.busy": "2024-11-15T15:37:54.216227Z",
     "iopub.status.idle": "2024-11-15T15:37:58.025643Z",
     "shell.execute_reply": "2024-11-15T15:37:58.025060Z",
     "shell.execute_reply.started": "2024-11-15T15:37:54.216599Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('#（一）有效提升通信网络覆盖深度广度\\n\\n加快推进“双千兆”网络建设。', 0.9829549)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"有效提升通信网络覆盖深度广度的措施有哪些？\"\n",
    "candidate_answer = Hybrid_retriever(query)\n",
    "reranked_answer = get_rerank3(query, candidate_answer)\n",
    "reranked_text = [n.text for n in reranked_answer]\n",
    "merge_answer = get_merge(reranked_text)\n",
    "final_answer = pruning3(query, merge_answer)\n",
    "final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b695a97c-86b6-4682-a81f-d63e1a727384",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-11-15T15:31:45.565816Z",
     "iopub.status.busy": "2024-11-15T15:31:45.565429Z",
     "iopub.status.idle": "2024-11-15T15:31:45.570016Z",
     "shell.execute_reply": "2024-11-15T15:31:45.569471Z",
     "shell.execute_reply.started": "2024-11-15T15:31:45.565790Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"#（一）有效提升通信网络覆盖深度广度\\n\\n加快推进“双千兆”网络建设。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c62fcedc-e68b-48c1-a36b-6a3e787f9be0",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-11-15T15:37:09.502863Z",
     "iopub.status.busy": "2024-11-15T15:37:09.502487Z",
     "iopub.status.idle": "2024-11-15T15:37:09.506831Z",
     "shell.execute_reply": "2024-11-15T15:37:09.506223Z",
     "shell.execute_reply.started": "2024-11-15T15:37:09.502840Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'迫切需要把握宽带体系化演进趋势，立足长远做好谋篇布局，发挥政府和市场、中央和地方、国有资本和社会资本的多轮驱动作用，适度超前推动宽带设施演进升级，建用互促深化融合创新，供需互动提高基础设施全生命周期综合效益，不断巩固提升既有优势，加快打通经济社会发展信息“大动脉”。\\n\\n#（一）有效提升通信网络覆盖深度广度\\n\\n加快推进“双千兆”网络建设。'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b74926a3-c957-4177-afd8-480961976a60",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-11-15T13:38:18.930525Z",
     "iopub.status.busy": "2024-11-15T13:38:18.930104Z",
     "iopub.status.idle": "2024-11-15T13:38:18.934889Z",
     "shell.execute_reply": "2024-11-15T13:38:18.934249Z",
     "shell.execute_reply.started": "2024-11-15T13:38:18.930500Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'坚持贯彻新发展理念，践行5G高质量发展。5G中频基站新增22万站，4G共享基站超过200万站。与友商在新疆启动全球首个5G异网漫游试商用，进一步提升边远地区网络资源使用效益；努力推动传输线路、管道光缆等基础设施的共建共享共维取得积极成效。以实际行动响应“双碳”政策，2023年节省运营开支约人民币390亿元，减少碳排放约1,150万吨，累计节省资本开支约人民币3,400亿元。'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "ecf98886-0369-40f1-9c01-136c1a949274",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-11-15T15:36:32.888473Z",
     "iopub.status.busy": "2024-11-15T15:36:32.888087Z",
     "iopub.status.idle": "2024-11-15T15:36:32.893491Z",
     "shell.execute_reply": "2024-11-15T15:36:32.892939Z",
     "shell.execute_reply.started": "2024-11-15T15:36:32.888448Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_path: /mnt/workspace/基于运营商文本数据的知识库检索/B_document/BZ02.pdf\n",
      "file_name: BZ02.pdf\n",
      "file_type: application/pdf\n",
      "file_size: 4589584\n",
      "creation_date: 2024-11-14\n",
      "last_modified_date: 2024-08-19\n",
      "发布时间: \n",
      "发布人:\n",
      "\n",
      "迫切需要把握宽带体系化演进趋势，立足长远做好谋篇布局，发挥政府和市场、中央和地方、国有资本和社会资本的多轮驱动作用，适度超前推动宽带设施演进升级，建用互促深化融合创新，供需互动提高基础设施全生命周期综合效益，不断巩固提升既有优势，加快打通经济社会发展信息“大动脉”。\n",
      "\n",
      "#（一）有效提升通信网络覆盖深度广度\n",
      "\n",
      "加快推进“双千兆”网络建设。\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 7f38f704-ee6a-4b8f-a4b8-a0b47e53a22d<br>**Similarity:** 0.9719997644424438<br>**Text:** 迫切需要把握宽带体系化演进趋势，立足长远做好谋篇布局，发挥政府和市场、中央和地方、国有资本和社会资本的多轮驱动作用，适度超前推动宽带设施演进升级，建用互促深化融合创新，供需互动提高基础设施全生命周期综合效益，不断巩固提升既有优势，加快打通经济社会发展信息“大动脉”。\n",
       "\n",
       "#（一）有效提升通信网络覆盖深度广度\n",
       "\n",
       "加快推进“双千兆”网络建设。<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "\n",
    "for retrieved_node in reranked_answer:\n",
    "    print(retrieved_node.get_content(metadata_mode=\"all\"))\n",
    "    display_source_node(retrieved_node, source_length=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "98776a81-0425-4186-858e-cf0ab673186c",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-11-14T15:56:36.037422Z",
     "iopub.status.busy": "2024-11-14T15:56:36.037076Z",
     "iopub.status.idle": "2024-11-14T15:56:36.041510Z",
     "shell.execute_reply": "2024-11-14T15:56:36.040933Z",
     "shell.execute_reply.started": "2024-11-14T15:56:36.037399Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_path': '/mnt/workspace/基于运营商文本数据的知识库检索/B_document/BY01.pdf',\n",
       " 'file_name': 'BY01.pdf',\n",
       " 'file_type': 'application/pdf',\n",
       " 'file_size': 821719,\n",
       " 'creation_date': '2024-11-14',\n",
       " 'last_modified_date': '2024-08-12',\n",
       " '发布时间': '',\n",
       " '发布人': '',\n",
       " 'document_title': '中国联通2023年：业绩稳健攀升，业务全面升级报告',\n",
       " 'questions_this_excerpt_can_answer': '1. 中国联通在2023年采取了哪些措施以响应“双碳”政策，并在节能减排方面取得了哪些成效？\\n2. 中国联通在2023年如何通过科技创新推动产业创新，并具体提升了哪些方面的能力？\\n3. 中国联通在提升社会责任和公司治理方面采取了哪些具体措施，并在哪些方面获得了行业认可？',\n",
       " 'excerpt_keywords': '5G发展, 节能减排, 科技创新, 产业升级, 资本运营, 人工智能, 大模型, 数据中心, 社会责任, 公司治理'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reranked_answer[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7006fc4b-d187-489e-b690-8aa6288fddd4",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-11-14T15:49:08.571974Z",
     "iopub.status.busy": "2024-11-14T15:49:08.571595Z",
     "iopub.status.idle": "2024-11-14T15:49:08.575322Z",
     "shell.execute_reply": "2024-11-14T15:49:08.574771Z",
     "shell.execute_reply.started": "2024-11-14T15:49:08.571950Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "坚持贯彻新发展理念，践行5G高质量发展。5G中频基站新增22万站，4G共享基站超过200万站。与友商在新疆启动全球首个5G异网漫游试商用，进一步提升边远地区网络资源使用效益；努力推动传输线路、管道光缆等基础设施的共建共享共维取得积极成效。以实际行动响应“双碳”政策，2023年节省运营开支约人民币390亿元，减少碳排放约1,150万吨，累计节省资本开支约人民币3,400亿元。\n",
      "\n",
      "#创新驱动，深化转型蹄疾步稳\n",
      "\n",
      "公司积极锻造核心能力，持续强劲科技创新引擎，着力以科技创新推动产业创新，加快发展新质生产力。加大人才培育力度，科技创新人员占比达到40%；年内授权专利达到2,287件，同比增长37%。\n",
      "\n",
      "#中国联合网络通信股份有限公司\n",
      "\n",
      "#2023年年度报告\n",
      "\n",
      "提升资本运营效能，加大资本合作力度，设立云宙时代、联通数智医疗科技等合资公司，专精特新数字化服务能力进一步提升。\n",
      "\n",
      "公司将人工智能作为重要战略方向，加快布局人工智能新兴产业，结合中国联通在算网基础设施、丰富业务场景、广泛客户触点的优势，着力打造联通特色的大模型开发应用范式。在刚刚过去的世界移动通信大会上，公司推出“元景”大模型体系，根据不同功能需求布局多参数版本，推动大模型更好赋能实体经济应用场景。以AI+大数据全面提升客户服务水平，相关成果入选行业权威奖项“吴文俊人工智能科学技术奖”。\n",
      "\n",
      "进一步加快卫星通信统筹布局，积极推动天地一体通信产业化、体系化发展。\n",
      "\n",
      "#（五）社会责任和公司治理\n",
      "\n",
      "公司胸怀国之大者，主动服务国家战略，把经济责任与社会责任有机结合。过去一年，公司在环境、社会、公司治理等方面彰显担当，为满足人民群众美好信息生活需要和千行百业数字化网络化智能化需求贡献联通力量。公司持续锻造坚实可靠的安全网络，提升数据与信息安全能力，健全协同保障体系，筑牢国家网络安全屏障。践行“双碳”战略，积极打造技术先进、绿色低碳、安全可靠的新型数据中心。构建普惠便捷的数字社会，全面助力乡村振兴，数字乡村平台覆盖25.5万个行政村，服务逾2,600万村民。充分展现责任担当，在通信重保、抢险救灾、改革攻坚等大事要事中践行初心使命，顺利完成抢险救灾通信保障任务。持续提升上市公司质量，通过反向路演等活动加强与资本市场沟通交流，积极回应股东关切，推动内在价值与市场价值共同提升。\n",
      "\n",
      "公司持续完善治理机制，为持续健康发展提供坚实保障，连续获中国上市公司协会“董事会最佳实践”“公司治理最佳实践”“董事会秘书最高评价”“董事会办公室最佳实践”“业绩说明会最佳实践”等肯定。\n",
      "\n",
      "#（六）未来展望\n",
      "\n",
      "当前，数字化、智能化、绿色化浪潮势不可挡，新一轮科技革命和产业变革迅猛发展，催生前所未有的新服务、新模式和新业态。在新的形势下，信息通信业的战略性、基础性、先导性作用愈发凸显。\n"
     ]
    }
   ],
   "source": [
    "print(reranked_text[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
